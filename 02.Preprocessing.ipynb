{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26fbd167",
   "metadata": {},
   "source": [
    "# Load ng·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdba101",
   "metadata": {},
   "source": [
    "Chu·∫©n b·ªã ƒë∆∞·ªùng d·∫´n l∆∞u preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db5fe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·∫µn s√†ng x·ª≠ l√Ω d·ªØ li·ªáu.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c d·ªØ li·ªáu g·ªëc v√† th∆∞ m·ª•c l∆∞u d·ªØ li·ªáu s·∫°ch\n",
    "INPUT_DIR = \"./UIT-ViHSD\"\n",
    "OUTPUT_DIR = \"./UIT-ViHSD-preprocessed\"\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a t·ªìn t·∫°i\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"ƒê√£ t·∫°o th∆∞ m·ª•c: {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"S·∫µn s√†ng x·ª≠ l√Ω d·ªØ li·ªáu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8009db07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape g·ªëc: (24048, 2)\n",
      "Dev shape g·ªëc:   (2672, 2)\n",
      "Test shape g·ªëc:  (6680, 2)\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªçc d·ªØ li·ªáu\n",
    "try:\n",
    "    df_train = pd.read_csv(os.path.join(INPUT_DIR, \"train.csv\"))\n",
    "    df_dev = pd.read_csv(os.path.join(INPUT_DIR, \"dev.csv\"))\n",
    "    df_test = pd.read_csv(os.path.join(INPUT_DIR, \"test.csv\"))\n",
    "    \n",
    "    print(f\"Train shape g·ªëc: {df_train.shape}\")\n",
    "    print(f\"Dev shape g·ªëc:   {df_dev.shape}\")\n",
    "    print(f\"Test shape g·ªëc:  {df_test.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu. H√£y ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7979a",
   "metadata": {},
   "source": [
    "# 01. Word-segmentating texts into words by the pyvi tool8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88773ed",
   "metadata": {},
   "source": [
    "X·ª≠ l√Ω s∆° b·ªô & ƒê·ªãnh nghƒ©a h√†m T√°ch t·ª´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831acf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train: ƒê√£ lo·∫°i b·ªè 2 d√≤ng Null.\n",
      "- Dev: ƒê√£ lo·∫°i b·ªè 0 d√≤ng Null.\n",
      "- Test: ƒê√£ lo·∫°i b·ªè 0 d√≤ng Null.\n",
      "\n",
      "‚úÖ ƒê√£ chu·∫©n b·ªã xong h√†m x·ª≠ l√Ω.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12836\\2941027614.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['free_text'] = df['free_text'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "from pyvi import ViTokenizer\n",
    "# 1. X·ª≠ l√Ω Missing Values tr∆∞·ªõc ƒë·ªÉ tr√°nh l·ªói khi t√°ch t·ª´\n",
    "def drop_missing(df, name):\n",
    "    initial = len(df)\n",
    "    df = df.dropna(subset=['free_text'])\n",
    "    # Chuy·ªÉn v·ªÅ string\n",
    "    df['free_text'] = df['free_text'].astype(str) \n",
    "    print(f\"- {name}: ƒê√£ lo·∫°i b·ªè {initial - len(df)} d√≤ng Null.\")\n",
    "    return df\n",
    "\n",
    "df_train = drop_missing(df_train, \"Train\")\n",
    "df_dev = drop_missing(df_dev, \"Dev\")\n",
    "df_test = drop_missing(df_test, \"Test\")\n",
    "\n",
    "# 2. ƒê·ªãnh nghƒ©a h√†m t√°ch t·ª´ an to√†n\n",
    "def safe_segmentation(text):\n",
    "    try:\n",
    "        # PyVi th·ª±c hi·ªán t√°ch t·ª´: \"sinh vi√™n\" -> \"sinh_vi√™n\"\n",
    "        return ViTokenizer.tokenize(text)\n",
    "    except Exception as e:\n",
    "        return text # Tr·∫£ v·ªÅ text g·ªëc n·∫øu l·ªói\n",
    "\n",
    "print(\"\\n‚úÖ ƒê√£ chu·∫©n b·ªã xong h√†m x·ª≠ l√Ω.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5e2652",
   "metadata": {},
   "source": [
    "Th·ª±c thi T√°ch t·ª´ (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf92cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ƒêang t√°ch t·ª´ t·∫≠p TRAIN (vui l√≤ng ƒë·ª£i)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ƒêang t√°ch t·ª´ t·∫≠p DEV...\n",
      "‚è≥ ƒêang t√°ch t·ª´ t·∫≠p TEST...\n",
      "‚úÖ Ho√†n t·∫•t t√°ch t·ª´.\n"
     ]
    }
   ],
   "source": [
    "print(\"‚è≥ ƒêang t√°ch t·ª´ t·∫≠p TRAIN (vui l√≤ng ƒë·ª£i)...\")\n",
    "df_train['free_text'] = df_train['free_text'].apply(safe_segmentation)\n",
    "\n",
    "print(\"‚è≥ ƒêang t√°ch t·ª´ t·∫≠p DEV...\")\n",
    "df_dev['free_text'] = df_dev['free_text'].apply(safe_segmentation)\n",
    "\n",
    "print(\"‚è≥ ƒêang t√°ch t·ª´ t·∫≠p TEST...\")\n",
    "df_test['free_text'] = df_test['free_text'].apply(safe_segmentation)\n",
    "\n",
    "print(\"‚úÖ Ho√†n t·∫•t t√°ch t·ª´.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462bf3f8",
   "metadata": {},
   "source": [
    "L∆∞u k·∫øt qu·∫£ v√†o UIT-ViHSD-preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73629c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu v√†o: ./UIT-ViHSD-preprocessed\n",
      "   1. ./UIT-ViHSD-preprocessed\\train_segmented.csv\n",
      "   2. ./UIT-ViHSD-preprocessed\\dev_segmented.csv\n",
      "   3. ./UIT-ViHSD-preprocessed\\test_segmented.csv\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªãnh nghƒ©a t√™n file ƒë·∫ßu ra\n",
    "out_train = os.path.join(OUTPUT_DIR, \"train_segmented.csv\")\n",
    "out_dev = os.path.join(OUTPUT_DIR, \"dev_segmented.csv\")\n",
    "out_test = os.path.join(OUTPUT_DIR, \"test_segmented.csv\")\n",
    "\n",
    "# L∆∞u file\n",
    "df_train.to_csv(out_train, index=False)\n",
    "df_dev.to_csv(out_dev, index=False)\n",
    "df_test.to_csv(out_test, index=False)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu v√†o: {OUTPUT_DIR}\")\n",
    "print(f\"   1. {out_train}\")\n",
    "print(f\"   2. {out_dev}\")\n",
    "print(f\"   3. {out_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a4b8b",
   "metadata": {},
   "source": [
    "Ki·ªÉm tra l·∫°i d·ªØ li·ªáu (Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de6ec0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- M·∫´u d·ªØ li·ªáu sau khi t√°ch t·ª´ (Train) ---\n",
      "                                                                                                          free_text  \\\n",
      "0                 Em ƒë∆∞·ª£c l√†m fan c·ª©ng lu√¥n r·ªìi n√® ‚ù§ Ô∏è reaction qu√° hay qu√° cute coi m·∫•y gi·ªù n√†y qu√° h·ª£p_l√≠ = ] ] ]   \n",
      "1  ƒê√∫ng l√† b·ªçn m·∫Øt h√≠p l√≤_xo th·ª•t : ) ) ) b√™n vi·ªát_nam t c√°i n√†y ra c√°ch ƒë√¢y 10 nƒÉm r v√† b·ªçn t g·ªçi l√† c√°i L : ) ) )   \n",
      "2                                                                          ƒê·∫≠u_VƒÉn C∆∞·ªùng gi·ªù gi·ªëng th·∫±ng sida h∆°n √†   \n",
      "3                                                  C√îN_ƒê·ªí C·ª§C_S√öC V√î NH√ÇN_T√çNH ƒê·ªÄ NGHI VN. NH√Ä_N∆Ø·ªöC VN BAN TH∆Ø·ªûNG .   \n",
      "4                                                           T·ª´ l√Ω_thuy·∫øt ƒë·∫øn th·ª±c_h√†nh l√† c·∫£ 1 c√¢u_chuy·ªán d√†i = ) )   \n",
      "\n",
      "   label_id  \n",
      "0         0  \n",
      "1         2  \n",
      "2         0  \n",
      "3         2  \n",
      "4         0  \n"
     ]
    }
   ],
   "source": [
    "print(\"--- M·∫´u d·ªØ li·ªáu sau khi t√°ch t·ª´ (Train) ---\")\n",
    "# Hi·ªÉn th·ªã c·ªôt text r·ªông h∆°n ƒë·ªÉ d·ªÖ nh√¨n\n",
    "pd.set_option('display.max_colwidth', 150) \n",
    "print(df_train[['free_text', 'label_id']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e14d46",
   "metadata": {},
   "source": [
    "So s√°nh v·ªõi d·ªØ li·ªáu c≈©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3851265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëÄ ƒêang l·∫•y m·∫´u ƒë·ªÉ so s√°nh...\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ORIGINAL (G·ªëc)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "SEGMENTED (ƒê√£ t√°ch t·ª´)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "68708f68-cef8-45b0-b19c-4bed6e0e621b",
       "rows": [
        [
         "0",
         "Lu√¢n TRr·∫ßn ch·ªõ k ph·∫£i l√† b·∫Øn pub h·∫£ y√™u chi kh·ªï.. a lo ·ªïm ƒë·ªãnh ƒëi r c∆∞·ªõi lu√¥n kh·ªèi y√™u cho tk e n√†y chi·∫øc ex üòÇüòÇ",
         "Lu√¢n TRr·∫ßn ch·ªõ k ph·∫£i l√† b·∫Øn pub h·∫£ y√™u chi kh·ªï . . a lo ·ªïm ƒë·ªãnh ƒëi r c∆∞·ªõi lu√¥n kh·ªèi y√™u cho tk e n√†y chi·∫øc ex üòÇ üòÇ",
         "0"
        ],
        [
         "1",
         "Th·∫±ng",
         "Th·∫±ng",
         "0"
        ],
        [
         "2",
         "Ng∆∞·ªùi ieu cua ai b·∫Øt v·ªÅ d√πm ƒëi c·∫£m ∆°n ...cho n√≥ h√∫t kh√≠ bot dien",
         "Ng∆∞·ªùi ieu cua ai b·∫Øt v·ªÅ d√πm ƒëi c·∫£m_∆°n ... cho n√≥ h√∫t kh√≠ bot dien",
         "0"
        ],
        [
         "3",
         "Bi·∫øt th·∫ø l√† ƒëc r. D·∫´u sao c≈©ng nh·∫Øc nh·ªü a e l√†. ƒê·ªçc th√¥i c√°i g√¨ c·∫ßn n√≥i h√£y n√≥i. C√°i th·∫±ng admin kia n√≥ k hi·ªÉu vƒë·ªÅ g√¨ ph√°n ng n√†y l√† ch√≥ ng kia l√† ch√≥. N√≥ l·∫°i hi·ªÉu ƒëc ti·∫øng ch√≥ :-?...",
         "Bi·∫øt th·∫ø_l√† ƒëc r . D·∫´u_sao c≈©ng nh·∫Øc_nh·ªü a e l√† . ƒê·ªçc th√¥i c√°i g√¨ c·∫ßn n√≥i h√£y n√≥i . C√°i th·∫±ng admin kia n√≥ k hi·ªÉu vƒë·ªÅ g√¨ ph√°n ng n√†y l√† ch√≥ ng kia l√† ch√≥ . N√≥ l·∫°i hi·ªÉu ƒëc ti·∫øng ch√≥ : - ? ...",
         "2"
        ],
        [
         "4",
         "T√∫ Nguy·ªÖn  n∆∞·ªõc hoa m√† t√≠nh v√†i trƒÉm ng√†n th√¨ l·∫•y ƒë√¢u ra lo·∫°i t·ªët b.",
         "T√∫ Nguy·ªÖn n∆∞·ªõc_hoa m√† t√≠nh v√†i trƒÉm ng√†n th√¨ l·∫•y ƒë√¢u ra lo·∫°i t·ªët b .",
         "0"
        ],
        [
         "5",
         "C√≤n ban s√°ng h·ªçp giao ban c·ªßa C·ª§C C·ª®C",
         "C√≤n ban s√°ng h·ªçp giao_ban c·ªßa C·ª§C C·ª®C",
         "1"
        ],
        [
         "6",
         "ƒêm boÃ£n thuy√™ÃÅt phuÃ£c xl",
         "ƒêm b·ªçn thuy·∫øt_ph·ª•c xl",
         "2"
        ],
        [
         "7",
         "Thanh L√Ω Sim s·ªë ƒë·∫πp--Giao d·ªãch to√†n qu·ªëc",
         "Thanh_L√Ω_Sim s·ªë ƒë·∫πp - - Giao_d·ªãch to√†n_qu·ªëc",
         "0"
        ],
        [
         "8",
         "Sting v√¥ ƒë·∫ßu",
         "Sting v√¥ ƒë·∫ßu",
         "0"
        ],
        [
         "9",
         "C·∫£m th·∫•y ch·ª≠i lƒëao l√† 1 th√∫ vui x·∫£ stress do b·ªçn n√≥ qu√° ngu v√† nguy hi·ªÉm :)))",
         "C·∫£m_th·∫•y ch·ª≠i lƒëao l√† 1 th√∫_vui x·∫£ stress do b·ªçn n√≥ qu√° ngu v√† nguy_hi·ªÉm : ) ) )",
         "2"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORIGINAL (G·ªëc)</th>\n",
       "      <th>SEGMENTED (ƒê√£ t√°ch t·ª´)</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lu√¢n TRr·∫ßn ch·ªõ k ph·∫£i l√† b·∫Øn pub h·∫£ y√™u chi kh·ªï.. a lo ·ªïm ƒë·ªãnh ƒëi r c∆∞·ªõi lu√¥n kh·ªèi y√™u cho tk e n√†y chi·∫øc ex üòÇüòÇ</td>\n",
       "      <td>Lu√¢n TRr·∫ßn ch·ªõ k ph·∫£i l√† b·∫Øn pub h·∫£ y√™u chi kh·ªï . . a lo ·ªïm ƒë·ªãnh ƒëi r c∆∞·ªõi lu√¥n kh·ªèi y√™u cho tk e n√†y chi·∫øc ex üòÇ üòÇ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Th·∫±ng</td>\n",
       "      <td>Th·∫±ng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ng∆∞·ªùi ieu cua ai b·∫Øt v·ªÅ d√πm ƒëi c·∫£m ∆°n ...cho n√≥ h√∫t kh√≠ bot dien</td>\n",
       "      <td>Ng∆∞·ªùi ieu cua ai b·∫Øt v·ªÅ d√πm ƒëi c·∫£m_∆°n ... cho n√≥ h√∫t kh√≠ bot dien</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bi·∫øt th·∫ø l√† ƒëc r. D·∫´u sao c≈©ng nh·∫Øc nh·ªü a e l√†. ƒê·ªçc th√¥i c√°i g√¨ c·∫ßn n√≥i h√£y n√≥i. C√°i th·∫±ng admin kia n√≥ k hi·ªÉu vƒë·ªÅ g√¨ ph√°n ng n√†y l√† ch√≥ ng kia l√† ch√≥. N√≥ l·∫°i hi·ªÉu ƒëc ti·∫øng ch√≥ :-?...</td>\n",
       "      <td>Bi·∫øt th·∫ø_l√† ƒëc r . D·∫´u_sao c≈©ng nh·∫Øc_nh·ªü a e l√† . ƒê·ªçc th√¥i c√°i g√¨ c·∫ßn n√≥i h√£y n√≥i . C√°i th·∫±ng admin kia n√≥ k hi·ªÉu vƒë·ªÅ g√¨ ph√°n ng n√†y l√† ch√≥ ng kia l√† ch√≥ . N√≥ l·∫°i hi·ªÉu ƒëc ti·∫øng ch√≥ : - ? ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T√∫ Nguy·ªÖn  n∆∞·ªõc hoa m√† t√≠nh v√†i trƒÉm ng√†n th√¨ l·∫•y ƒë√¢u ra lo·∫°i t·ªët b.</td>\n",
       "      <td>T√∫ Nguy·ªÖn n∆∞·ªõc_hoa m√† t√≠nh v√†i trƒÉm ng√†n th√¨ l·∫•y ƒë√¢u ra lo·∫°i t·ªët b .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C√≤n ban s√°ng h·ªçp giao ban c·ªßa C·ª§C C·ª®C</td>\n",
       "      <td>C√≤n ban s√°ng h·ªçp giao_ban c·ªßa C·ª§C C·ª®C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ƒêm boÃ£n thuy√™ÃÅt phuÃ£c xl</td>\n",
       "      <td>ƒêm b·ªçn thuy·∫øt_ph·ª•c xl</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Thanh L√Ω Sim s·ªë ƒë·∫πp--Giao d·ªãch to√†n qu·ªëc</td>\n",
       "      <td>Thanh_L√Ω_Sim s·ªë ƒë·∫πp - - Giao_d·ªãch to√†n_qu·ªëc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sting v√¥ ƒë·∫ßu</td>\n",
       "      <td>Sting v√¥ ƒë·∫ßu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C·∫£m th·∫•y ch·ª≠i lƒëao l√† 1 th√∫ vui x·∫£ stress do b·ªçn n√≥ qu√° ngu v√† nguy hi·ªÉm :)))</td>\n",
       "      <td>C·∫£m_th·∫•y ch·ª≠i lƒëao l√† 1 th√∫_vui x·∫£ stress do b·ªçn n√≥ qu√° ngu v√† nguy_hi·ªÉm : ) ) )</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                            ORIGINAL (G·ªëc)  \\\n",
       "0                                                                          Lu√¢n TRr·∫ßn ch·ªõ k ph·∫£i l√† b·∫Øn pub h·∫£ y√™u chi kh·ªï.. a lo ·ªïm ƒë·ªãnh ƒëi r c∆∞·ªõi lu√¥n kh·ªèi y√™u cho tk e n√†y chi·∫øc ex üòÇüòÇ   \n",
       "1                                                                                                                                                                                    Th·∫±ng   \n",
       "2                                                                                                                         Ng∆∞·ªùi ieu cua ai b·∫Øt v·ªÅ d√πm ƒëi c·∫£m ∆°n ...cho n√≥ h√∫t kh√≠ bot dien   \n",
       "3  Bi·∫øt th·∫ø l√† ƒëc r. D·∫´u sao c≈©ng nh·∫Øc nh·ªü a e l√†. ƒê·ªçc th√¥i c√°i g√¨ c·∫ßn n√≥i h√£y n√≥i. C√°i th·∫±ng admin kia n√≥ k hi·ªÉu vƒë·ªÅ g√¨ ph√°n ng n√†y l√† ch√≥ ng kia l√† ch√≥. N√≥ l·∫°i hi·ªÉu ƒëc ti·∫øng ch√≥ :-?...   \n",
       "4                                                                                                                     T√∫ Nguy·ªÖn  n∆∞·ªõc hoa m√† t√≠nh v√†i trƒÉm ng√†n th√¨ l·∫•y ƒë√¢u ra lo·∫°i t·ªët b.   \n",
       "5                                                                                                                                                    C√≤n ban s√°ng h·ªçp giao ban c·ªßa C·ª§C C·ª®C   \n",
       "6                                                                                                                                                                 ƒêm boÃ£n thuy√™ÃÅt phuÃ£c xl   \n",
       "7                                                                                                                                                 Thanh L√Ω Sim s·ªë ƒë·∫πp--Giao d·ªãch to√†n qu·ªëc   \n",
       "8                                                                                                                                                                             Sting v√¥ ƒë·∫ßu   \n",
       "9                                                                                                            C·∫£m th·∫•y ch·ª≠i lƒëao l√† 1 th√∫ vui x·∫£ stress do b·ªçn n√≥ qu√° ngu v√† nguy hi·ªÉm :)))   \n",
       "\n",
       "                                                                                                                                                                           SEGMENTED (ƒê√£ t√°ch t·ª´)  \\\n",
       "0                                                                              Lu√¢n TRr·∫ßn ch·ªõ k ph·∫£i l√† b·∫Øn pub h·∫£ y√™u chi kh·ªï . . a lo ·ªïm ƒë·ªãnh ƒëi r c∆∞·ªõi lu√¥n kh·ªèi y√™u cho tk e n√†y chi·∫øc ex üòÇ üòÇ   \n",
       "1                                                                                                                                                                                           Th·∫±ng   \n",
       "2                                                                                                                               Ng∆∞·ªùi ieu cua ai b·∫Øt v·ªÅ d√πm ƒëi c·∫£m_∆°n ... cho n√≥ h√∫t kh√≠ bot dien   \n",
       "3  Bi·∫øt th·∫ø_l√† ƒëc r . D·∫´u_sao c≈©ng nh·∫Øc_nh·ªü a e l√† . ƒê·ªçc th√¥i c√°i g√¨ c·∫ßn n√≥i h√£y n√≥i . C√°i th·∫±ng admin kia n√≥ k hi·ªÉu vƒë·ªÅ g√¨ ph√°n ng n√†y l√† ch√≥ ng kia l√† ch√≥ . N√≥ l·∫°i hi·ªÉu ƒëc ti·∫øng ch√≥ : - ? ...   \n",
       "4                                                                                                                            T√∫ Nguy·ªÖn n∆∞·ªõc_hoa m√† t√≠nh v√†i trƒÉm ng√†n th√¨ l·∫•y ƒë√¢u ra lo·∫°i t·ªët b .   \n",
       "5                                                                                                                                                           C√≤n ban s√°ng h·ªçp giao_ban c·ªßa C·ª§C C·ª®C   \n",
       "6                                                                                                                                                                           ƒêm b·ªçn thuy·∫øt_ph·ª•c xl   \n",
       "7                                                                                                                                                     Thanh_L√Ω_Sim s·ªë ƒë·∫πp - - Giao_d·ªãch to√†n_qu·ªëc   \n",
       "8                                                                                                                                                                                    Sting v√¥ ƒë·∫ßu   \n",
       "9                                                                                                                C·∫£m_th·∫•y ch·ª≠i lƒëao l√† 1 th√∫_vui x·∫£ stress do b·ªçn n√≥ qu√° ngu v√† nguy_hi·ªÉm : ) ) )   \n",
       "\n",
       "   Label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      2  \n",
       "4      0  \n",
       "5      1  \n",
       "6      2  \n",
       "7      0  \n",
       "8      0  \n",
       "9      2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "RAW_FILE = \"./UIT-ViHSD/train.csv\"\n",
    "SEG_FILE = \"./UIT-ViHSD-preprocessed/train_segmented.csv\"\n",
    "\n",
    "def compare_segmentation(n_samples=10):\n",
    "    print(\"üëÄ ƒêang l·∫•y m·∫´u ƒë·ªÉ so s√°nh...\")\n",
    "    \n",
    "    # 1. ƒê·ªçc d·ªØ li·ªáu\n",
    "    try:\n",
    "        df_raw = pd.read_csv(RAW_FILE)\n",
    "        df_seg = pd.read_csv(SEG_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Kh√¥ng t√¨m th·∫•y file. H√£y ch·∫Øc ch·∫Øn b·∫°n ƒë√£ ch·∫°y c√°c b∆∞·ªõc tr∆∞·ªõc ƒë√≥.\")\n",
    "        return\n",
    "\n",
    "    # 2. ƒê·ªìng b·ªô h√≥a d·ªØ li·ªáu (v√¨ file segmented ƒë√£ b·ªã drop Null)\n",
    "    # Ta c·∫ßn drop Null ·ªü file Raw t∆∞∆°ng t·ª± ƒë·ªÉ index kh·ªõp nhau\n",
    "    df_raw = df_raw.dropna(subset=['free_text']).reset_index(drop=True)\n",
    "    df_seg = df_seg.reset_index(drop=True)\n",
    "    \n",
    "    # 3. L·∫•y m·∫´u ng·∫´u nhi√™n\n",
    "    # Ch·ªçn c√°c c√¢u c√≥ ƒë·ªô d√†i trung b√¨nh (ƒë·ªÉ d·ªÖ nh√¨n)\n",
    "    # df_seg['len'] = df_seg['free_text'].str.len()\n",
    "    # sample_indices = df_seg[(df_seg['len'] > 50) & (df_seg['len'] < 150)].sample(n_samples).index\n",
    "    \n",
    "    # Ho·∫∑c l·∫•y ng·∫´u nhi√™n ho√†n to√†n\n",
    "    if len(df_seg) > n_samples:\n",
    "        sample_indices = df_seg.sample(n_samples, random_state=42).index\n",
    "    else:\n",
    "        sample_indices = df_seg.index\n",
    "\n",
    "    # 4. T·∫°o b·∫£ng so s√°nh\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'ORIGINAL (G·ªëc)': df_raw.loc[sample_indices, 'free_text'].values,\n",
    "        'SEGMENTED (ƒê√£ t√°ch t·ª´)': df_seg.loc[sample_indices, 'free_text'].values,\n",
    "        'Label': df_seg.loc[sample_indices, 'label_id'].values\n",
    "    })\n",
    "    \n",
    "    # 5. Hi·ªÉn th·ªã ƒë·∫πp\n",
    "    pd.set_option('display.max_colwidth', None) # Hi·ªÉn th·ªã to√†n b·ªô n·ªôi dung\n",
    "    display(comparison_df) # D√πng display() trong Jupyter ƒë·ªÉ ƒë·∫πp h∆°n print()\n",
    "\n",
    "# G·ªçi h√†m\n",
    "compare_segmentation(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098be00",
   "metadata": {},
   "source": [
    "# 02. Lo·∫°i b·ªè stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a461b8c",
   "metadata": {},
   "source": [
    "Khai b√°o ƒë∆∞·ªùng d·∫´n v√† Load Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec2cc2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ load 1942 t·ª´ d·ª´ng (stopwords).\n",
      "V√≠ d·ª• v√†i t·ª´ trong set: ['b·∫±ng_nh∆∞', 'g√¨_ƒë√≥', 't·ªõi_m·ª©c', 'nh√†_chung', 'ch·∫Øc_ch·∫Øn', 'quan_tr·ªçng', 'c√†ng', 'ƒë·∫ßu_ti√™n', 'thi_tho·∫£ng', 'l√†m_g√¨']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ---\n",
    "INPUT_DIR = \"./UIT-ViHSD-preprocessed\" # L·∫•y d·ªØ li·ªáu t·ª´ b∆∞·ªõc tr∆∞·ªõc\n",
    "STOPWORD_FILE = \"vietnamese-stopwords.txt\" # File stopwords c·ªßa b·∫°n\n",
    "\n",
    "# --- H√ÄM LOAD STOPWORDS ---\n",
    "def load_stopwords(filepath):\n",
    "    stopwords = set()\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    # QUAN TR·ªåNG: Chuy·ªÉn stopword th√†nh d·∫°ng segmented (thay d·∫•u c√°ch b·∫±ng _)\n",
    "                    # V√≠ d·ª•: \"m·∫∑c d√π\" -> \"m·∫∑c_d√π\" ƒë·ªÉ kh·ªõp v·ªõi d·ªØ li·ªáu ƒë√£ t√°ch t·ª´\n",
    "                    word_segmented = word.replace(' ', '_')\n",
    "                    stopwords.add(word_segmented)\n",
    "        print(f\"‚úÖ ƒê√£ load {len(stopwords)} t·ª´ d·ª´ng (stopwords).\")\n",
    "        return stopwords\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file {filepath}. Vui l√≤ng ki·ªÉm tra l·∫°i.\")\n",
    "        return set()\n",
    "\n",
    "# Load stopwords v√†o b·ªô nh·ªõ\n",
    "stopword_set = load_stopwords(STOPWORD_FILE)\n",
    "\n",
    "# Ki·ªÉm tra th·ª≠ v√†i t·ª´\n",
    "if len(stopword_set) > 0:\n",
    "    print(f\"V√≠ d·ª• v√†i t·ª´ trong set: {list(stopword_set)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "583021f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ load d·ªØ li·ªáu segmented th√†nh c√¥ng.\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªçc c√°c file ƒë√£ segmented\n",
    "try:\n",
    "    df_train = pd.read_csv(os.path.join(INPUT_DIR, \"train_segmented.csv\"))\n",
    "    df_dev = pd.read_csv(os.path.join(INPUT_DIR, \"dev_segmented.csv\"))\n",
    "    df_test = pd.read_csv(os.path.join(INPUT_DIR, \"test_segmented.csv\"))\n",
    "    \n",
    "    # X·ª≠ l√Ω NaN n·∫øu c√≥ (do qu√° tr√¨nh ƒë·ªçc ghi file)\n",
    "    df_train['free_text'] = df_train['free_text'].fillna(\"\").astype(str)\n",
    "    df_dev['free_text'] = df_dev['free_text'].fillna(\"\").astype(str)\n",
    "    df_test['free_text'] = df_test['free_text'].fillna(\"\").astype(str)\n",
    "\n",
    "    print(\"‚úÖ ƒê√£ load d·ªØ li·ªáu segmented th√†nh c√¥ng.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu segmented. H√£y ch·∫°y b∆∞·ªõc t√°ch t·ª´ tr∆∞·ªõc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a2a663e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a h√†m remove_stopwords.\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords_from_text(text, stopword_set):\n",
    "    \"\"\"\n",
    "    Input: \"H√¥m_nay t√¥i ƒëi h·ªçc\"\n",
    "    Stopwords: {\"t√¥i\", \"ƒëi\"}\n",
    "    Output: \"H√¥m_nay h·ªçc\"\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # T√°ch c√¢u th√†nh list c√°c token (d·ª±a v√†o kho·∫£ng tr·∫Øng)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # L·ªçc b·ªè token n·∫øu n√≥ n·∫±m trong stopword_set\n",
    "    clean_tokens = [t for t in tokens if t.lower() not in stopword_set]\n",
    "    \n",
    "    # N·ªëi l·∫°i th√†nh chu·ªói\n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a h√†m remove_stopwords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "119c16c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ƒêang lo·∫°i b·ªè stopwords cho t·∫≠p Train...\n",
      "‚è≥ ƒêang lo·∫°i b·ªè stopwords cho t·∫≠p Dev...\n",
      "‚è≥ ƒêang lo·∫°i b·ªè stopwords cho t·∫≠p Test...\n",
      "‚úÖ Ho√†n t·∫•t.\n"
     ]
    }
   ],
   "source": [
    "if len(stopword_set) > 0:\n",
    "    print(\"‚è≥ ƒêang lo·∫°i b·ªè stopwords cho t·∫≠p Train...\")\n",
    "    df_train['free_text'] = df_train['free_text'].apply(lambda x: remove_stopwords_from_text(x, stopword_set))\n",
    "\n",
    "    print(\"‚è≥ ƒêang lo·∫°i b·ªè stopwords cho t·∫≠p Dev...\")\n",
    "    df_dev['free_text'] = df_dev['free_text'].apply(lambda x: remove_stopwords_from_text(x, stopword_set))\n",
    "\n",
    "    print(\"‚è≥ ƒêang lo·∫°i b·ªè stopwords cho t·∫≠p Test...\")\n",
    "    df_test['free_text'] = df_test['free_text'].apply(lambda x: remove_stopwords_from_text(x, stopword_set))\n",
    "    \n",
    "    print(\"‚úÖ Ho√†n t·∫•t.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è C·∫£nh b√°o: T·∫≠p stopword r·ªóng, b·ªè qua b∆∞·ªõc n√†y.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83b9cd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- M·∫´u d·ªØ li·ªáu sau khi b·ªè Stopwords (Train) ---\n",
      "                                                            free_text  \\\n",
      "0                fan c·ª©ng n√® ‚ù§ Ô∏è reaction cute coi m·∫•y h·ª£p_l√≠ = ] ] ]   \n",
      "1  b·ªçn m·∫Øt h√≠p l√≤_xo th·ª•t : ) ) ) vi·ªát_nam t 10 r b·ªçn t g·ªçi L : ) ) )   \n",
      "2                                            ƒê·∫≠u_VƒÉn C∆∞·ªùng th·∫±ng sida   \n",
      "3    C√îN_ƒê·ªí C·ª§C_S√öC V√î NH√ÇN_T√çNH ƒê·ªÄ NGHI VN. NH√Ä_N∆Ø·ªöC VN BAN TH∆Ø·ªûNG .   \n",
      "4                              l√Ω_thuy·∫øt th·ª±c_h√†nh 1 c√¢u_chuy·ªán = ) )   \n",
      "\n",
      "   label_id  \n",
      "0         0  \n",
      "1         2  \n",
      "2         0  \n",
      "3         2  \n",
      "4         0  \n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu s·∫°ch v√†o folder ./UIT-ViHSD-preprocessed:\n",
      "   - ./UIT-ViHSD-preprocessed\\train_no_sw.csv\n",
      "   - ./UIT-ViHSD-preprocessed\\dev_no_sw.csv\n",
      "   - ./UIT-ViHSD-preprocessed\\test_no_sw.csv\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra k·∫øt qu·∫£\n",
    "print(\"--- M·∫´u d·ªØ li·ªáu sau khi b·ªè Stopwords (Train) ---\")\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "print(df_train[['free_text', 'label_id']].head(5))\n",
    "\n",
    "# L∆∞u file\n",
    "out_train = os.path.join(INPUT_DIR, \"train_no_sw.csv\")\n",
    "out_dev = os.path.join(INPUT_DIR, \"dev_no_sw.csv\")\n",
    "out_test = os.path.join(INPUT_DIR, \"test_no_sw.csv\")\n",
    "\n",
    "df_train.to_csv(out_train, index=False)\n",
    "df_dev.to_csv(out_dev, index=False)\n",
    "df_test.to_csv(out_test, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu s·∫°ch v√†o folder {INPUT_DIR}:\")\n",
    "print(f\"   - {out_train}\")\n",
    "print(f\"   - {out_dev}\")\n",
    "print(f\"   - {out_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd1834c",
   "metadata": {},
   "source": [
    "Ki·ªÉm tra d·ªØ li·ªáu tr∆∞·ªõc v√† sau khi b·ªè stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d80c7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´: ./UIT-ViHSD-preprocessed/train_segmented.csv\n",
      "‚úÖ ƒê√£ t√¨m th·∫•y 20 m·∫´u c√≥ s·ª± thay ƒë·ªïi.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Original (Segmented)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Cleaned",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "‚ùå REMOVED WORDS",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "7238aef1-e62a-4517-a91d-8c52bdc2a2e1",
       "rows": [
        [
         "0",
         "Em ƒë∆∞·ª£c l√†m fan c·ª©ng lu√¥n r·ªìi n√® ‚ù§ Ô∏è reaction qu√° hay qu√° cute coi m·∫•y gi·ªù n√†y qu√° h·ª£p_l√≠ = ] ] ]",
         "fan c·ª©ng n√® ‚ù§ Ô∏è reaction cute coi m·∫•y h·ª£p_l√≠ = ] ] ]",
         "gi·ªù, hay, r·ªìi, n√†y, qu√°, Em, l√†m, lu√¥n, ƒë∆∞·ª£c",
         "0"
        ],
        [
         "1",
         "ƒê√∫ng l√† b·ªçn m·∫Øt h√≠p l√≤_xo th·ª•t : ) ) ) b√™n vi·ªát_nam t c√°i n√†y ra c√°ch ƒë√¢y 10 nƒÉm r v√† b·ªçn t g·ªçi l√† c√°i L : ) ) )",
         "b·ªçn m·∫Øt h√≠p l√≤_xo th·ª•t : ) ) ) vi·ªát_nam t 10 r b·ªçn t g·ªçi L : ) ) )",
         "b√™n, c√°ch, l√†, c√°i, ƒë√¢y, n√†y, nƒÉm, v√†, ƒê√∫ng, ra",
         "2"
        ],
        [
         "2",
         "ƒê·∫≠u_VƒÉn C∆∞·ªùng gi·ªù gi·ªëng th·∫±ng sida h∆°n √†",
         "ƒê·∫≠u_VƒÉn C∆∞·ªùng th·∫±ng sida",
         "gi·ªëng, √†, gi·ªù, h∆°n",
         "0"
        ],
        [
         "3",
         "T·ª´ l√Ω_thuy·∫øt ƒë·∫øn th·ª±c_h√†nh l√† c·∫£ 1 c√¢u_chuy·ªán d√†i = ) )",
         "l√Ω_thuy·∫øt th·ª±c_h√†nh 1 c√¢u_chuy·ªán = ) )",
         "l√†, c·∫£, T·ª´, d√†i, ƒë·∫øn",
         "0"
        ],
        [
         "4",
         "ƒê·ªë ch√∫ng m nh·∫≠n ra ai",
         "ƒê·ªë m",
         "ch√∫ng, ra, nh·∫≠n, ai",
         "0"
        ],
        [
         "5",
         "L√∫p l√∫p nh∆∞ ch√≥ .",
         "L√∫p l√∫p ch√≥ .",
         "nh∆∞",
         "1"
        ],
        [
         "6",
         "Th·∫ø_m√† m√¨nh n√≥i m·∫•y th·∫±ng b·∫Øc k√¨ , b·ªçn ƒë√≥ l·∫°i b√¢u v√¥ n√≥i m√¨nh ph√¢n_bi·ªát ! : D",
         "m·∫•y th·∫±ng b·∫Øc k√¨ , b·ªçn b√¢u v√¥ ph√¢n_bi·ªát ! : D",
         "Th·∫ø_m√†, m√¨nh, l·∫°i, ƒë√≥, n√≥i",
         "2"
        ],
        [
         "7",
         "ƒê∆∞·ª£c anh ∆∞i , l√¢u r·ªìi kh√¥ng nghe ph√∫c du rap ü§£",
         "∆∞i , ph√∫c du rap ü§£",
         "ƒê∆∞·ª£c, r·ªìi, nghe, anh, l√¢u, kh√¥ng",
         "0"
        ],
        [
         "8",
         "C·∫Øt cho tr·∫ª tr√¢u b·ªõt thui m√† üòÇ üòÇ üòÇ",
         "C·∫Øt tr·∫ª tr√¢u b·ªõt thui üòÇ üòÇ üòÇ",
         "cho, m√†",
         "0"
        ],
        [
         "9",
         "Nghe r·∫•t v√¥_l√≠ nh∆∞ng c·ª±c_k√¨ thuy·∫øt_ph·ª•c",
         "v√¥_l√≠ c·ª±c_k√¨ thuy·∫øt_ph·ª•c",
         "Nghe, nh∆∞ng, r·∫•t",
         "0"
        ],
        [
         "10",
         "ch√°n gh√™ hong c√≥ p√© 2 m·ªõi m√°u ƒë∆∞·ª£c , h√¢hha",
         "ch√°n gh√™ hong p√© 2 m√°u , h√¢hha",
         "c√≥, ƒë∆∞·ª£c, m·ªõi",
         "0"
        ],
        [
         "11",
         "Sao t g·ª≠i ƒëc b√¢y",
         "t g·ª≠i ƒëc b√¢y",
         "Sao",
         "0"
        ],
        [
         "12",
         "th·∫ø ƒë·∫•y . l√†m j b·ªçn n√≥ . b·ªçn n√≥ c√≥ quy·ªÅn n√™n l√†m j ch·∫£ ƒë∆∞·ª£c : v",
         "ƒë·∫•y . j b·ªçn . b·ªçn quy·ªÅn j ch·∫£ : v",
         "th·∫ø, c√≥, n√™n, n√≥, l√†m, ƒë∆∞·ª£c",
         "0"
        ],
        [
         "13",
         "Lo·∫°i n√†y cho d·ª±a c·ªôt th√¥i ch·ª© ƒë·ªô th·∫ø_n√†o",
         "d·ª±a c·ªôt ƒë·ªô",
         "cho, Lo·∫°i, th·∫ø_n√†o, n√†y, th√¥i, ch·ª©",
         "2"
        ],
        [
         "14",
         "D·∫°y b∆°i cho c√° . B∆°i th√¨ ƒë∆∞∆°ng_nhi√™n n√≥ b∆°i ƒë∆∞·ª£c v√¨ n√≥ l√† c√° . L√†m_sao cho n√≥ s·ªëng ƒë∆∞·ª£c m·ªõi l√† ƒëi·ªÅu ng∆∞·ªùi d√¢n ƒëang quan_t√¢m .",
         "D·∫°y b∆°i c√° . B∆°i ƒë∆∞∆°ng_nhi√™n b∆°i c√° . s·ªëng d√¢n .",
         "cho, ƒëi·ªÅu, l√†, ƒëang, th√¨, quan_t√¢m, ng∆∞·ªùi, m·ªõi, v√¨, n√≥, ƒë∆∞·ª£c, L√†m_sao",
         "1"
        ],
        [
         "15",
         "·ª¶a ch·ª© b·ªØa xin_l·ªói kh√≥c t·∫ø om x√≤m m√† gi·ªù ch·ª≠i n·ªØa r·ªìi b√≥_tay con n√†y h·ªón qu√°",
         "b·ªØa xin_l·ªói kh√≥c t·∫ø om x√≤m ch·ª≠i b√≥_tay h·ªón",
         "n·ªØa, gi·ªù, r·ªìi, con, n√†y, qu√°, m√†, ch·ª©, ·ª¶a",
         "2"
        ],
        [
         "16",
         "Bamtp ch·∫°y ngay ddi : ) )",
         "Bamtp ch·∫°y ddi : ) )",
         "ngay",
         "0"
        ],
        [
         "17",
         "C·ª© ƒëƒÉng c√°i ·∫£nh 2 b·ªë con l√† y_nh∆∞_r·∫±ng b·ªã ƒë·ªïi",
         "ƒëƒÉng ·∫£nh 2 b·ªë y_nh∆∞_r·∫±ng ƒë·ªïi",
         "b·ªã, c√°i, l√†, con, C·ª©",
         "0"
        ],
        [
         "18",
         "D√†n T√°o nƒÉm nay l√†m ch∆∞∆°ng_tr√¨nh T√ÅO_QU√ÇN VI H√ÄNH nh√© mn , ph√°t_s√≥ng 9h t·ªëi 23 T·∫øt tr√™n VTV3 : ) )",
         "D√†n T√°o ch∆∞∆°ng_tr√¨nh T√ÅO_QU√ÇN VI H√ÄNH mn , ph√°t_s√≥ng 9h t·ªëi 23 T·∫øt VTV3 : ) )",
         "nay, tr√™n, nh√©, nƒÉm, l√†m",
         "0"
        ],
        [
         "19",
         "Th·∫≠t ch√°n v·ªõi ch·∫ø_ƒë·ªô qu·∫£n_l√Ω d√¢n_sinh",
         "ch√°n ch·∫ø_ƒë·ªô qu·∫£n_l√Ω d√¢n_sinh",
         "v·ªõi, Th·∫≠t",
         "2"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original (Segmented)</th>\n",
       "      <th>Cleaned</th>\n",
       "      <th>‚ùå REMOVED WORDS</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Em ƒë∆∞·ª£c l√†m fan c·ª©ng lu√¥n r·ªìi n√® ‚ù§ Ô∏è reaction qu√° hay qu√° cute coi m·∫•y gi·ªù n√†y qu√° h·ª£p_l√≠ = ] ] ]</td>\n",
       "      <td>fan c·ª©ng n√® ‚ù§ Ô∏è reaction cute coi m·∫•y h·ª£p_l√≠ = ] ] ]</td>\n",
       "      <td>gi·ªù, hay, r·ªìi, n√†y, qu√°, Em, l√†m, lu√¥n, ƒë∆∞·ª£c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ƒê√∫ng l√† b·ªçn m·∫Øt h√≠p l√≤_xo th·ª•t : ) ) ) b√™n vi·ªát_nam t c√°i n√†y ra c√°ch ƒë√¢y 10 nƒÉm r v√† b·ªçn t g·ªçi l√† c√°i L : ) ) )</td>\n",
       "      <td>b·ªçn m·∫Øt h√≠p l√≤_xo th·ª•t : ) ) ) vi·ªát_nam t 10 r b·ªçn t g·ªçi L : ) ) )</td>\n",
       "      <td>b√™n, c√°ch, l√†, c√°i, ƒë√¢y, n√†y, nƒÉm, v√†, ƒê√∫ng, ra</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ƒê·∫≠u_VƒÉn C∆∞·ªùng gi·ªù gi·ªëng th·∫±ng sida h∆°n √†</td>\n",
       "      <td>ƒê·∫≠u_VƒÉn C∆∞·ªùng th·∫±ng sida</td>\n",
       "      <td>gi·ªëng, √†, gi·ªù, h∆°n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T·ª´ l√Ω_thuy·∫øt ƒë·∫øn th·ª±c_h√†nh l√† c·∫£ 1 c√¢u_chuy·ªán d√†i = ) )</td>\n",
       "      <td>l√Ω_thuy·∫øt th·ª±c_h√†nh 1 c√¢u_chuy·ªán = ) )</td>\n",
       "      <td>l√†, c·∫£, T·ª´, d√†i, ƒë·∫øn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ƒê·ªë ch√∫ng m nh·∫≠n ra ai</td>\n",
       "      <td>ƒê·ªë m</td>\n",
       "      <td>ch√∫ng, ra, nh·∫≠n, ai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L√∫p l√∫p nh∆∞ ch√≥ .</td>\n",
       "      <td>L√∫p l√∫p ch√≥ .</td>\n",
       "      <td>nh∆∞</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Th·∫ø_m√† m√¨nh n√≥i m·∫•y th·∫±ng b·∫Øc k√¨ , b·ªçn ƒë√≥ l·∫°i b√¢u v√¥ n√≥i m√¨nh ph√¢n_bi·ªát ! : D</td>\n",
       "      <td>m·∫•y th·∫±ng b·∫Øc k√¨ , b·ªçn b√¢u v√¥ ph√¢n_bi·ªát ! : D</td>\n",
       "      <td>Th·∫ø_m√†, m√¨nh, l·∫°i, ƒë√≥, n√≥i</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ƒê∆∞·ª£c anh ∆∞i , l√¢u r·ªìi kh√¥ng nghe ph√∫c du rap ü§£</td>\n",
       "      <td>∆∞i , ph√∫c du rap ü§£</td>\n",
       "      <td>ƒê∆∞·ª£c, r·ªìi, nghe, anh, l√¢u, kh√¥ng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C·∫Øt cho tr·∫ª tr√¢u b·ªõt thui m√† üòÇ üòÇ üòÇ</td>\n",
       "      <td>C·∫Øt tr·∫ª tr√¢u b·ªõt thui üòÇ üòÇ üòÇ</td>\n",
       "      <td>cho, m√†</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nghe r·∫•t v√¥_l√≠ nh∆∞ng c·ª±c_k√¨ thuy·∫øt_ph·ª•c</td>\n",
       "      <td>v√¥_l√≠ c·ª±c_k√¨ thuy·∫øt_ph·ª•c</td>\n",
       "      <td>Nghe, nh∆∞ng, r·∫•t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ch√°n gh√™ hong c√≥ p√© 2 m·ªõi m√°u ƒë∆∞·ª£c , h√¢hha</td>\n",
       "      <td>ch√°n gh√™ hong p√© 2 m√°u , h√¢hha</td>\n",
       "      <td>c√≥, ƒë∆∞·ª£c, m·ªõi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sao t g·ª≠i ƒëc b√¢y</td>\n",
       "      <td>t g·ª≠i ƒëc b√¢y</td>\n",
       "      <td>Sao</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>th·∫ø ƒë·∫•y . l√†m j b·ªçn n√≥ . b·ªçn n√≥ c√≥ quy·ªÅn n√™n l√†m j ch·∫£ ƒë∆∞·ª£c : v</td>\n",
       "      <td>ƒë·∫•y . j b·ªçn . b·ªçn quy·ªÅn j ch·∫£ : v</td>\n",
       "      <td>th·∫ø, c√≥, n√™n, n√≥, l√†m, ƒë∆∞·ª£c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Lo·∫°i n√†y cho d·ª±a c·ªôt th√¥i ch·ª© ƒë·ªô th·∫ø_n√†o</td>\n",
       "      <td>d·ª±a c·ªôt ƒë·ªô</td>\n",
       "      <td>cho, Lo·∫°i, th·∫ø_n√†o, n√†y, th√¥i, ch·ª©</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>D·∫°y b∆°i cho c√° . B∆°i th√¨ ƒë∆∞∆°ng_nhi√™n n√≥ b∆°i ƒë∆∞·ª£c v√¨ n√≥ l√† c√° . L√†m_sao cho n√≥ s·ªëng ƒë∆∞·ª£c m·ªõi l√† ƒëi·ªÅu ng∆∞·ªùi d√¢n ƒëang quan_t√¢m .</td>\n",
       "      <td>D·∫°y b∆°i c√° . B∆°i ƒë∆∞∆°ng_nhi√™n b∆°i c√° . s·ªëng d√¢n .</td>\n",
       "      <td>cho, ƒëi·ªÅu, l√†, ƒëang, th√¨, quan_t√¢m, ng∆∞·ªùi, m·ªõi, v√¨, n√≥, ƒë∆∞·ª£c, L√†m_sao</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>·ª¶a ch·ª© b·ªØa xin_l·ªói kh√≥c t·∫ø om x√≤m m√† gi·ªù ch·ª≠i n·ªØa r·ªìi b√≥_tay con n√†y h·ªón qu√°</td>\n",
       "      <td>b·ªØa xin_l·ªói kh√≥c t·∫ø om x√≤m ch·ª≠i b√≥_tay h·ªón</td>\n",
       "      <td>n·ªØa, gi·ªù, r·ªìi, con, n√†y, qu√°, m√†, ch·ª©, ·ª¶a</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bamtp ch·∫°y ngay ddi : ) )</td>\n",
       "      <td>Bamtp ch·∫°y ddi : ) )</td>\n",
       "      <td>ngay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>C·ª© ƒëƒÉng c√°i ·∫£nh 2 b·ªë con l√† y_nh∆∞_r·∫±ng b·ªã ƒë·ªïi</td>\n",
       "      <td>ƒëƒÉng ·∫£nh 2 b·ªë y_nh∆∞_r·∫±ng ƒë·ªïi</td>\n",
       "      <td>b·ªã, c√°i, l√†, con, C·ª©</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>D√†n T√°o nƒÉm nay l√†m ch∆∞∆°ng_tr√¨nh T√ÅO_QU√ÇN VI H√ÄNH nh√© mn , ph√°t_s√≥ng 9h t·ªëi 23 T·∫øt tr√™n VTV3 : ) )</td>\n",
       "      <td>D√†n T√°o ch∆∞∆°ng_tr√¨nh T√ÅO_QU√ÇN VI H√ÄNH mn , ph√°t_s√≥ng 9h t·ªëi 23 T·∫øt VTV3 : ) )</td>\n",
       "      <td>nay, tr√™n, nh√©, nƒÉm, l√†m</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Th·∫≠t ch√°n v·ªõi ch·∫ø_ƒë·ªô qu·∫£n_l√Ω d√¢n_sinh</td>\n",
       "      <td>ch√°n ch·∫ø_ƒë·ªô qu·∫£n_l√Ω d√¢n_sinh</td>\n",
       "      <td>v·ªõi, Th·∫≠t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                             Original (Segmented)  \\\n",
       "0                               Em ƒë∆∞·ª£c l√†m fan c·ª©ng lu√¥n r·ªìi n√® ‚ù§ Ô∏è reaction qu√° hay qu√° cute coi m·∫•y gi·ªù n√†y qu√° h·ª£p_l√≠ = ] ] ]   \n",
       "1                ƒê√∫ng l√† b·ªçn m·∫Øt h√≠p l√≤_xo th·ª•t : ) ) ) b√™n vi·ªát_nam t c√°i n√†y ra c√°ch ƒë√¢y 10 nƒÉm r v√† b·ªçn t g·ªçi l√† c√°i L : ) ) )   \n",
       "2                                                                                        ƒê·∫≠u_VƒÉn C∆∞·ªùng gi·ªù gi·ªëng th·∫±ng sida h∆°n √†   \n",
       "3                                                                         T·ª´ l√Ω_thuy·∫øt ƒë·∫øn th·ª±c_h√†nh l√† c·∫£ 1 c√¢u_chuy·ªán d√†i = ) )   \n",
       "4                                                                                                           ƒê·ªë ch√∫ng m nh·∫≠n ra ai   \n",
       "5                                                                                                               L√∫p l√∫p nh∆∞ ch√≥ .   \n",
       "6                                                   Th·∫ø_m√† m√¨nh n√≥i m·∫•y th·∫±ng b·∫Øc k√¨ , b·ªçn ƒë√≥ l·∫°i b√¢u v√¥ n√≥i m√¨nh ph√¢n_bi·ªát ! : D   \n",
       "7                                                                                  ƒê∆∞·ª£c anh ∆∞i , l√¢u r·ªìi kh√¥ng nghe ph√∫c du rap ü§£   \n",
       "8                                                                                              C·∫Øt cho tr·∫ª tr√¢u b·ªõt thui m√† üòÇ üòÇ üòÇ   \n",
       "9                                                                                         Nghe r·∫•t v√¥_l√≠ nh∆∞ng c·ª±c_k√¨ thuy·∫øt_ph·ª•c   \n",
       "10                                                                                     ch√°n gh√™ hong c√≥ p√© 2 m·ªõi m√°u ƒë∆∞·ª£c , h√¢hha   \n",
       "11                                                                                                               Sao t g·ª≠i ƒëc b√¢y   \n",
       "12                                                                th·∫ø ƒë·∫•y . l√†m j b·ªçn n√≥ . b·ªçn n√≥ c√≥ quy·ªÅn n√™n l√†m j ch·∫£ ƒë∆∞·ª£c : v   \n",
       "13                                                                                       Lo·∫°i n√†y cho d·ª±a c·ªôt th√¥i ch·ª© ƒë·ªô th·∫ø_n√†o   \n",
       "14  D·∫°y b∆°i cho c√° . B∆°i th√¨ ƒë∆∞∆°ng_nhi√™n n√≥ b∆°i ƒë∆∞·ª£c v√¨ n√≥ l√† c√° . L√†m_sao cho n√≥ s·ªëng ƒë∆∞·ª£c m·ªõi l√† ƒëi·ªÅu ng∆∞·ªùi d√¢n ƒëang quan_t√¢m .   \n",
       "15                                                   ·ª¶a ch·ª© b·ªØa xin_l·ªói kh√≥c t·∫ø om x√≤m m√† gi·ªù ch·ª≠i n·ªØa r·ªìi b√≥_tay con n√†y h·ªón qu√°   \n",
       "16                                                                                                      Bamtp ch·∫°y ngay ddi : ) )   \n",
       "17                                                                                  C·ª© ƒëƒÉng c√°i ·∫£nh 2 b·ªë con l√† y_nh∆∞_r·∫±ng b·ªã ƒë·ªïi   \n",
       "18                             D√†n T√°o nƒÉm nay l√†m ch∆∞∆°ng_tr√¨nh T√ÅO_QU√ÇN VI H√ÄNH nh√© mn , ph√°t_s√≥ng 9h t·ªëi 23 T·∫øt tr√™n VTV3 : ) )   \n",
       "19                                                                                          Th·∫≠t ch√°n v·ªõi ch·∫ø_ƒë·ªô qu·∫£n_l√Ω d√¢n_sinh   \n",
       "\n",
       "                                                                          Cleaned  \\\n",
       "0                            fan c·ª©ng n√® ‚ù§ Ô∏è reaction cute coi m·∫•y h·ª£p_l√≠ = ] ] ]   \n",
       "1              b·ªçn m·∫Øt h√≠p l√≤_xo th·ª•t : ) ) ) vi·ªát_nam t 10 r b·ªçn t g·ªçi L : ) ) )   \n",
       "2                                                        ƒê·∫≠u_VƒÉn C∆∞·ªùng th·∫±ng sida   \n",
       "3                                          l√Ω_thuy·∫øt th·ª±c_h√†nh 1 c√¢u_chuy·ªán = ) )   \n",
       "4                                                                            ƒê·ªë m   \n",
       "5                                                                   L√∫p l√∫p ch√≥ .   \n",
       "6                                   m·∫•y th·∫±ng b·∫Øc k√¨ , b·ªçn b√¢u v√¥ ph√¢n_bi·ªát ! : D   \n",
       "7                                                              ∆∞i , ph√∫c du rap ü§£   \n",
       "8                                                     C·∫Øt tr·∫ª tr√¢u b·ªõt thui üòÇ üòÇ üòÇ   \n",
       "9                                                        v√¥_l√≠ c·ª±c_k√¨ thuy·∫øt_ph·ª•c   \n",
       "10                                                 ch√°n gh√™ hong p√© 2 m√°u , h√¢hha   \n",
       "11                                                                   t g·ª≠i ƒëc b√¢y   \n",
       "12                                              ƒë·∫•y . j b·ªçn . b·ªçn quy·ªÅn j ch·∫£ : v   \n",
       "13                                                                     d·ª±a c·ªôt ƒë·ªô   \n",
       "14                               D·∫°y b∆°i c√° . B∆°i ƒë∆∞∆°ng_nhi√™n b∆°i c√° . s·ªëng d√¢n .   \n",
       "15                                     b·ªØa xin_l·ªói kh√≥c t·∫ø om x√≤m ch·ª≠i b√≥_tay h·ªón   \n",
       "16                                                           Bamtp ch·∫°y ddi : ) )   \n",
       "17                                                   ƒëƒÉng ·∫£nh 2 b·ªë y_nh∆∞_r·∫±ng ƒë·ªïi   \n",
       "18  D√†n T√°o ch∆∞∆°ng_tr√¨nh T√ÅO_QU√ÇN VI H√ÄNH mn , ph√°t_s√≥ng 9h t·ªëi 23 T·∫øt VTV3 : ) )   \n",
       "19                                                   ch√°n ch·∫ø_ƒë·ªô qu·∫£n_l√Ω d√¢n_sinh   \n",
       "\n",
       "                                                          ‚ùå REMOVED WORDS  \\\n",
       "0                            gi·ªù, hay, r·ªìi, n√†y, qu√°, Em, l√†m, lu√¥n, ƒë∆∞·ª£c   \n",
       "1                         b√™n, c√°ch, l√†, c√°i, ƒë√¢y, n√†y, nƒÉm, v√†, ƒê√∫ng, ra   \n",
       "2                                                      gi·ªëng, √†, gi·ªù, h∆°n   \n",
       "3                                                    l√†, c·∫£, T·ª´, d√†i, ƒë·∫øn   \n",
       "4                                                     ch√∫ng, ra, nh·∫≠n, ai   \n",
       "5                                                                     nh∆∞   \n",
       "6                                              Th·∫ø_m√†, m√¨nh, l·∫°i, ƒë√≥, n√≥i   \n",
       "7                                        ƒê∆∞·ª£c, r·ªìi, nghe, anh, l√¢u, kh√¥ng   \n",
       "8                                                                 cho, m√†   \n",
       "9                                                        Nghe, nh∆∞ng, r·∫•t   \n",
       "10                                                          c√≥, ƒë∆∞·ª£c, m·ªõi   \n",
       "11                                                                    Sao   \n",
       "12                                            th·∫ø, c√≥, n√™n, n√≥, l√†m, ƒë∆∞·ª£c   \n",
       "13                                     cho, Lo·∫°i, th·∫ø_n√†o, n√†y, th√¥i, ch·ª©   \n",
       "14  cho, ƒëi·ªÅu, l√†, ƒëang, th√¨, quan_t√¢m, ng∆∞·ªùi, m·ªõi, v√¨, n√≥, ƒë∆∞·ª£c, L√†m_sao   \n",
       "15                              n·ªØa, gi·ªù, r·ªìi, con, n√†y, qu√°, m√†, ch·ª©, ·ª¶a   \n",
       "16                                                                   ngay   \n",
       "17                                                   b·ªã, c√°i, l√†, con, C·ª©   \n",
       "18                                               nay, tr√™n, nh√©, nƒÉm, l√†m   \n",
       "19                                                              v·ªõi, Th·∫≠t   \n",
       "\n",
       "    Label  \n",
       "0       0  \n",
       "1       2  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "5       1  \n",
       "6       2  \n",
       "7       0  \n",
       "8       0  \n",
       "9       0  \n",
       "10      0  \n",
       "11      0  \n",
       "12      0  \n",
       "13      2  \n",
       "14      1  \n",
       "15      2  \n",
       "16      0  \n",
       "17      0  \n",
       "18      0  \n",
       "19      2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "INPUT_FILE = \"./UIT-ViHSD-preprocessed/train_segmented.csv\"\n",
    "\n",
    "# --- H√ÄM X·ª¨ L√ù ---\n",
    "def filter_impacted_rows(input_path, stopword_set, n_limit=20):\n",
    "    print(f\"üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´: {input_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Load d·ªØ li·ªáu g·ªëc (ƒë√£ t√°ch t·ª´ nh∆∞ng ch∆∞a x√≥a stopword)\n",
    "        df = pd.read_csv(input_path)\n",
    "        df['free_text'] = df['free_text'].fillna(\"\").astype(str)\n",
    "        \n",
    "        # Danh s√°ch ch·ª©a k·∫øt qu·∫£\n",
    "        impacted_rows = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            text = row['free_text']\n",
    "            tokens = text.split()\n",
    "            \n",
    "            # T√¨m c√°c t·ª´ b·ªã x√≥a\n",
    "            removed = [t for t in tokens if t.lower() in stopword_set]\n",
    "            \n",
    "            # Ch·ªâ l·∫•y nh·ªØng d√≤ng C√ì t·ª´ b·ªã x√≥a (removed list kh√¥ng r·ªóng)\n",
    "            if len(removed) > 0:\n",
    "                # T·∫°o l·∫°i c√¢u ƒë√£ clean ƒë·ªÉ so s√°nh\n",
    "                cleaned = [t for t in tokens if t.lower() not in stopword_set]\n",
    "                \n",
    "                impacted_rows.append({\n",
    "                    'Original (Segmented)': text,\n",
    "                    'Cleaned': \" \".join(cleaned),\n",
    "                    '‚ùå REMOVED WORDS': \", \".join(set(removed)), # Set ƒë·ªÉ lo·∫°i b·ªè t·ª´ tr√πng trong c√πng 1 c√¢u\n",
    "                    'Label': row['label_id']\n",
    "                })\n",
    "                \n",
    "            # D·ª´ng n·∫øu ƒë√£ l·∫•y ƒë·ªß s·ªë l∆∞·ª£ng m·∫´u c·∫ßn xem (ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian ch·∫°y demo)\n",
    "            if len(impacted_rows) >= n_limit:\n",
    "                break\n",
    "        \n",
    "        # T·∫°o DataFrame hi·ªÉn th·ªã\n",
    "        df_impact = pd.DataFrame(impacted_rows)\n",
    "        \n",
    "        print(f\"‚úÖ ƒê√£ t√¨m th·∫•y {len(df_impact)} m·∫´u c√≥ s·ª± thay ƒë·ªïi.\")\n",
    "        \n",
    "        # Hi·ªÉn th·ªã\n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "        display(df_impact)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Kh√¥ng t√¨m th·∫•y file segmented. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.\")\n",
    "\n",
    "# --- TH·ª∞C THI ---\n",
    "# Gi·∫£ ƒë·ªãnh bi·∫øn stopword_set ƒë√£ c√≥ t·ª´ b∆∞·ªõc tr∆∞·ªõc. \n",
    "# N·∫øu ch∆∞a c√≥, b·∫°n c·∫ßn ch·∫°y l·∫°i Cell load stopwords.\n",
    "if 'stopword_set' in globals() and len(stopword_set) > 0:\n",
    "    # L·∫•y 20 m·∫´u c√≥ thay ƒë·ªïi ƒë·ªÉ ki·ªÉm tra\n",
    "    filter_impacted_rows(INPUT_FILE, stopword_set, n_limit=20)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Bi·∫øn 'stopword_set' ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. Vui l√≤ng ch·∫°y l·∫°i Cell load stopwords ·ªü tr√™n.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b594e6",
   "metadata": {},
   "source": [
    "# 3. Changing all texts into lower cases10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eb4f928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu c√≥ ƒëu√¥i '_no_sw.csv'...\n",
      "‚úÖ ƒê√£ load xong 3 t·∫≠p d·ªØ li·ªáu.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "DIR = \"./UIT-ViHSD-preprocessed\"\n",
    "\n",
    "# ƒêu√¥i file ƒë·∫ßu v√†o (Ch·ªçn lo·∫°i file b·∫°n mu·ªën x·ª≠ l√Ω ti·∫øp)\n",
    "# N·∫øu b·∫°n ƒê√É ch·∫°y b·ªè stopwords -> d√πng \"_no_sw.csv\"\n",
    "# N·∫øu b·∫°n KH√îNG ch·∫°y b·ªè stopwords -> d√πng \"_segmented.csv\"\n",
    "INPUT_SUFFIX = \"_no_sw.csv\" \n",
    "\n",
    "# ƒêu√¥i file ƒë·∫ßu ra\n",
    "OUTPUT_SUFFIX = \"_lower.csv\"\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "try:\n",
    "    print(f\"üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu c√≥ ƒëu√¥i '{INPUT_SUFFIX}'...\")\n",
    "    df_train = pd.read_csv(os.path.join(DIR, f\"train{INPUT_SUFFIX}\"))\n",
    "    df_dev = pd.read_csv(os.path.join(DIR, f\"dev{INPUT_SUFFIX}\"))\n",
    "    df_test = pd.read_csv(os.path.join(DIR, f\"test{INPUT_SUFFIX}\"))\n",
    "    \n",
    "    # X·ª≠ l√Ω NaN ph√≤ng h·ªù\n",
    "    df_train['free_text'] = df_train['free_text'].fillna(\"\").astype(str)\n",
    "    df_dev['free_text'] = df_dev['free_text'].fillna(\"\").astype(str)\n",
    "    df_test['free_text'] = df_test['free_text'].fillna(\"\").astype(str)\n",
    "    \n",
    "    print(\"‚úÖ ƒê√£ load xong 3 t·∫≠p d·ªØ li·ªáu.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file v·ªõi ƒëu√¥i {INPUT_SUFFIX}. H√£y ki·ªÉm tra l·∫°i folder {DIR}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b1b4256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ƒêang chuy·ªÉn ƒë·ªïi sang ch·ªØ th∆∞·ªùng...\n",
      "‚úÖ Ho√†n t·∫•t chuy·ªÉn ƒë·ªïi.\n"
     ]
    }
   ],
   "source": [
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "print(\"‚è≥ ƒêang chuy·ªÉn ƒë·ªïi sang ch·ªØ th∆∞·ªùng...\")\n",
    "\n",
    "# √Åp d·ª•ng\n",
    "df_train['free_text'] = df_train['free_text'].apply(to_lowercase)\n",
    "df_dev['free_text'] = df_dev['free_text'].apply(to_lowercase)\n",
    "df_test['free_text'] = df_test['free_text'].apply(to_lowercase)\n",
    "\n",
    "print(\"‚úÖ Ho√†n t·∫•t chuy·ªÉn ƒë·ªïi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7369815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu lowercase v√†o: ./UIT-ViHSD-preprocessed\n",
      "   - ./UIT-ViHSD-preprocessed\\train_lower.csv\n",
      "   - ./UIT-ViHSD-preprocessed\\dev_lower.csv\n",
      "   - ./UIT-ViHSD-preprocessed\\test_lower.csv\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªãnh nghƒ©a t√™n file\n",
    "out_train = os.path.join(DIR, f\"train{OUTPUT_SUFFIX}\")\n",
    "out_dev = os.path.join(DIR, f\"dev{OUTPUT_SUFFIX}\")\n",
    "out_test = os.path.join(DIR, f\"test{OUTPUT_SUFFIX}\")\n",
    "\n",
    "# L∆∞u file\n",
    "df_train.to_csv(out_train, index=False)\n",
    "df_dev.to_csv(out_dev, index=False)\n",
    "df_test.to_csv(out_test, index=False)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu lowercase v√†o: {DIR}\")\n",
    "print(f\"   - {out_train}\")\n",
    "print(f\"   - {out_dev}\")\n",
    "print(f\"   - {out_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "177e141d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- M·∫´u d·ªØ li·ªáu sau khi Lowercase ---\n",
      "                                               free_text  label_id\n",
      "15159                                            hat wua         0\n",
      "15572      minh nh·∫≠t_√©o ch·∫øt d·ªãch ch·∫øt n·ªï ƒët ü§£ ü§£ ü§£ ü§£ ü§£ ü§£         1\n",
      "9080   tui t∆∞·ªüng b·ªçn h·ªçc_tr√≤ gh√©p clip ai_ng·ªù gi·∫£i thi·ªát         0\n",
      "22895                                                ch√≥         0\n",
      "19011                                       12 : 00 ch·ª≠i         0\n",
      "\n",
      "T·ªïng s·ªë k√Ω t·ª± in hoa c√≤n s√≥t l·∫°i trong t·∫≠p Train: 0 (K·ª≥ v·ªçng: 0)\n"
     ]
    }
   ],
   "source": [
    "# L·∫•y m·∫´u ng·∫´u nhi√™n ƒë·ªÉ ki·ªÉm tra\n",
    "print(\"--- M·∫´u d·ªØ li·ªáu sau khi Lowercase ---\")\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "print(df_train[['free_text', 'label_id']].sample(5))\n",
    "\n",
    "# Ki·ªÉm tra k·ªπ thu·∫≠t: ƒê·∫øm s·ªë k√Ω t·ª± in hoa c√≤n s√≥t l·∫°i\n",
    "upper_count = df_train['free_text'].str.count(r'[A-Z√Ä√Å√Ç√É√à√â√ä√å√ç√í√ì√î√ï√ô√ö√ùƒÇƒêƒ®≈®∆†∆Ø·∫†·∫¢·∫§·∫¶·∫®·∫™·∫¨·∫Æ·∫∞·∫≤·∫¥·∫∂·∫∏·∫∫·∫º·ªÄ·ªÄ·ªÇ·ªÑ·ªÜ·ªà·ªä·ªå·ªé·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢·ª§·ª¶·ª®·ª™·ª¨·ªÆ·ª∞·ª≤·ª¥·ª∂·ª∏]').sum()\n",
    "print(f\"\\nT·ªïng s·ªë k√Ω t·ª± in hoa c√≤n s√≥t l·∫°i trong t·∫≠p Train: {upper_count} (K·ª≥ v·ªçng: 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2376221",
   "metadata": {},
   "source": [
    "# 4. Lo·∫°i b·ªè c√°c ƒë·∫∑c tr∆∞ng Regex (Emoji, URL, Hashtag, Mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8154e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ b∆∞·ªõc Lowercase...\n",
      "‚è≥ ƒêang lo·∫°i b·ªè URL, Mention, Hashtag v√† k√Ω t·ª± ƒë·∫∑c bi·ªát...\n",
      "‚è≥ Ki·ªÉm tra d√≤ng r·ªóng sau khi clean...\n",
      "‚ö†Ô∏è C·∫£nh b√°o: C√≥ 152 d√≤ng tr·ªü n√™n r·ªóng ·ªü t·∫≠p Train. (S·∫Ω gi·ªØ nguy√™n ho·∫∑c ƒëi·ªÅn 'empty')\n",
      "\n",
      "‚úÖ HO√ÄN T·∫§T QU√Å TR√åNH PREPROCESSING!\n",
      "üìÇ D·ªØ li·ªáu s·∫°ch cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: ./UIT-ViHSD-preprocessed\n",
      "   1. ./UIT-ViHSD-preprocessed\\train.csv\n",
      "   2. ./UIT-ViHSD-preprocessed\\dev.csv\n",
      "   3. ./UIT-ViHSD-preprocessed\\test.csv\n",
      "\n",
      "--- M·∫´u d·ªØ li·ªáu cu·ªëi c√πng (Final) ---\n",
      "                                                            free_text  \\\n",
      "0                fan c·ª©ng n√® ‚ù§ Ô∏è reaction cute coi m·∫•y h·ª£p_l√≠ = ] ] ]   \n",
      "1  b·ªçn m·∫Øt h√≠p l√≤_xo th·ª•t : ) ) ) vi·ªát_nam t 10 r b·ªçn t g·ªçi l : ) ) )   \n",
      "2                                            ƒë·∫≠u_vƒÉn c∆∞·ªùng th·∫±ng sida   \n",
      "3    c√¥n_ƒë·ªì c·ª•c_s√∫c v√¥ nh√¢n_t√≠nh ƒë·ªÅ nghi vn. nh√†_n∆∞·ªõc vn ban th∆∞·ªüng .   \n",
      "4                              l√Ω_thuy·∫øt th·ª±c_h√†nh 1 c√¢u_chuy·ªán = ) )   \n",
      "\n",
      "   label_id  \n",
      "0         0  \n",
      "1         2  \n",
      "2         0  \n",
      "3         2  \n",
      "4         0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "DIR = \"./UIT-ViHSD-preprocessed\"\n",
    "# File ƒë·∫ßu v√†o: L·∫•y t·ª´ b∆∞·ªõc tr∆∞·ªõc (lower case)\n",
    "INPUT_SUFFIX = \"_lower.csv\" \n",
    "\n",
    "# --- H√ÄM L√ÄM S·∫†CH N√ÇNG CAO ---\n",
    "def clean_special_patterns(text):\n",
    "    if not isinstance(text, str):\n",
    "        return str(text)\n",
    "    \n",
    "    # 1. X·ª≠ l√Ω URL\n",
    "    # Regex b·∫Øt http/https/www\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # 2. X·ª≠ l√Ω Mention (@Username)\n",
    "    # B·∫Øt @ theo sau l√† c√°c k√Ω t·ª± kh√¥ng ph·∫£i kho·∫£ng tr·∫Øng\n",
    "    text = re.sub(r'@\\S+', ' ', text)\n",
    "    \n",
    "    # 3. X·ª≠ l√Ω Hashtag (#Hashtag)\n",
    "    # C√°ch 1: X√≥a h·∫≥n -> re.sub(r'#\\S+', '', text)\n",
    "    # C√°ch 2: Ch·ªâ x√≥a d·∫•u #, gi·ªØ l·∫°i ch·ªØ -> text.replace('#', '')\n",
    "    # D·ª±a tr√™n th·ªëng k√™ c·ªßa b·∫°n (#error, #name), t√¥i ch·ªçn x√≥a d·∫•u # ƒë·ªÉ gi·ªØ l·∫°i n·ªôi dung ch·ªØ n·∫øu c√≥ nghƒ©a\n",
    "    text = text.replace('#', '')\n",
    "    \n",
    "    # 4. X·ª≠ l√Ω c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát kh√°c (D·∫•u c√¢u th·ª´a, icon, emoji...)\n",
    "    # Gi·ªØ l·∫°i: Ch·ªØ c√°i (bao g·ªìm ti·∫øng Vi·ªát), s·ªë, v√† c√°c d·∫•u c√¢u c∆° b·∫£n\n",
    "    # Pattern n√†y gi·ªØ l·∫°i: \n",
    "    # - \\w: Ch·ªØ v√† s·ªë\n",
    "    # - \\s: Kho·∫£ng tr·∫Øng\n",
    "    # - C√°c d·∫•u c√¢u ph·ªï bi·∫øn: .,?!:;\"\"''\n",
    "    # C√°c k√Ω t·ª± l·∫° (emoji, symbol ƒë·∫∑c bi·ªát) s·∫Ω b·ªã lo·∫°i b·ªè\n",
    "    # L∆∞u √Ω: Python regex m·∫∑c ƒë·ªãnh \\w ch∆∞a ch·∫Øc b·∫Øt h·∫øt unicode ti·∫øng Vi·ªát n·∫øu kh√¥ng c·∫•u h√¨nh k·ªπ.\n",
    "    # N√™n d√πng c√°ch ph·ªß ƒë·ªãnh: X√≥a nh·ªØng g√¨ KH√îNG PH·∫¢I l√† word ho·∫∑c space ho·∫∑c punctuation\n",
    "    \n",
    "    # Tuy nhi√™n, ƒë∆°n gi·∫£n nh·∫•t l√† d√πng regex thay th·∫ø c√°c k√Ω t·ª± kh√¥ng mong mu·ªën\n",
    "    # ·ªû ƒë√¢y t√¥i s·∫Ω x√≥a emoji v√† c√°c k√Ω t·ª± symbol l·∫°, ch·ªâ gi·ªØ l·∫°i text v√† d·∫•u c√¢u c∆° b·∫£n\n",
    "    # pattern = r'[^\\w\\s,.?!:;\"\\']' # Gi·ªØ l·∫°i word, space v√† d·∫•u c√¢u c∆° b·∫£n\n",
    "    # text = re.sub(pattern, ' ', text)\n",
    "    \n",
    "    # X·ª≠ l√Ω kho·∫£ng tr·∫Øng th·ª´a (do vi·ªác x√≥a t·∫°o ra)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# --- LOAD D·ªÆ LI·ªÜU ---\n",
    "try:\n",
    "    print(\"‚è≥ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ b∆∞·ªõc Lowercase...\")\n",
    "    df_train = pd.read_csv(os.path.join(DIR, f\"train{INPUT_SUFFIX}\"))\n",
    "    df_dev = pd.read_csv(os.path.join(DIR, f\"dev{INPUT_SUFFIX}\"))\n",
    "    df_test = pd.read_csv(os.path.join(DIR, f\"test{INPUT_SUFFIX}\"))\n",
    "    \n",
    "    # Fillna\n",
    "    df_train['free_text'] = df_train['free_text'].fillna(\"\").astype(str)\n",
    "    df_dev['free_text'] = df_dev['free_text'].fillna(\"\").astype(str)\n",
    "    df_test['free_text'] = df_test['free_text'].fillna(\"\").astype(str)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file input. Vui l√≤ng ki·ªÉm tra l·∫°i b∆∞·ªõc tr∆∞·ªõc.\")\n",
    "\n",
    "# --- √ÅP D·ª§NG L√ÄM S·∫†CH ---\n",
    "print(\"‚è≥ ƒêang lo·∫°i b·ªè URL, Mention, Hashtag v√† k√Ω t·ª± ƒë·∫∑c bi·ªát...\")\n",
    "\n",
    "df_train['free_text'] = df_train['free_text'].apply(clean_special_patterns)\n",
    "df_dev['free_text'] = df_dev['free_text'].apply(clean_special_patterns)\n",
    "df_test['free_text'] = df_test['free_text'].apply(clean_special_patterns)\n",
    "\n",
    "# --- X·ª¨ L√ù L·∫†I N·∫æU C√ì D√íNG R·ªñNG ---\n",
    "# Sau khi x√≥a h·∫øt ƒë·∫∑c bi·ªát, c√≥ th·ªÉ c√≥ d√≤ng th√†nh r·ªóng\n",
    "print(\"‚è≥ Ki·ªÉm tra d√≤ng r·ªóng sau khi clean...\")\n",
    "empty_train = df_train[df_train['free_text'] == ''].shape[0]\n",
    "if empty_train > 0:\n",
    "    print(f\"‚ö†Ô∏è C·∫£nh b√°o: C√≥ {empty_train} d√≤ng tr·ªü n√™n r·ªóng ·ªü t·∫≠p Train. (S·∫Ω gi·ªØ nguy√™n ho·∫∑c ƒëi·ªÅn 'empty')\")\n",
    "    # T√πy ch·ªçn: df_train = df_train[df_train['free_text'] != '']\n",
    "    # Nh∆∞ng ƒë·ªÉ an to√†n cho vi·ªác kh·ªõp d√≤ng, ta n√™n gi·ªØ nguy√™n ho·∫∑c ƒëi·ªÅn placeholder\n",
    "    df_train.loc[df_train['free_text'] == '', 'free_text'] = 'empty_content' \n",
    "    df_dev.loc[df_dev['free_text'] == '', 'free_text'] = 'empty_content'\n",
    "    df_test.loc[df_test['free_text'] == '', 'free_text'] = 'empty_content'\n",
    "\n",
    "# --- L∆ØU FILE CU·ªêI C√ôNG (FINAL) ---\n",
    "# T√™n file chu·∫©n: train.csv, dev.csv, test.csv\n",
    "out_train = os.path.join(DIR, \"train.csv\")\n",
    "out_dev = os.path.join(DIR, \"dev.csv\")\n",
    "out_test = os.path.join(DIR, \"test.csv\")\n",
    "\n",
    "df_train.to_csv(out_train, index=False)\n",
    "df_dev.to_csv(out_dev, index=False)\n",
    "df_test.to_csv(out_test, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ HO√ÄN T·∫§T QU√Å TR√åNH PREPROCESSING!\")\n",
    "print(f\"üìÇ D·ªØ li·ªáu s·∫°ch cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {DIR}\")\n",
    "print(f\"   1. {out_train}\")\n",
    "print(f\"   2. {out_dev}\")\n",
    "print(f\"   3. {out_test}\")\n",
    "\n",
    "# In m·∫´u k·∫øt qu·∫£ cu·ªëi c√πng\n",
    "print(\"\\n--- M·∫´u d·ªØ li·ªáu cu·ªëi c√πng (Final) ---\")\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "print(df_train[['free_text', 'label_id']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c63f024",
   "metadata": {},
   "source": [
    "# Lo·∫°i b·ªè c√°c d√≤ng r·ªóng ·ªü t·∫≠p train sau b∆∞·ªõc lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d53c23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ƒêang ki·ªÉm tra file: ./UIT-ViHSD-preprocessed\\train.csv\n",
      "‚ö†Ô∏è ƒê√£ ph√°t hi·ªán v√† x√≥a 152 d√≤ng r·ªóng.\n",
      "‚úÖ ƒê√£ c·∫≠p nh·∫≠t file: ./UIT-ViHSD-preprocessed\\train.csv\n",
      "   - S·ªë l∆∞·ª£ng d√≤ng c√≤n l·∫°i: 23894\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "DIR = \"./UIT-ViHSD-preprocessed\"\n",
    "TRAIN_FILE = os.path.join(DIR, \"train.csv\")\n",
    "\n",
    "# --- H√ÄM X·ª¨ L√ù ---\n",
    "def remove_empty_rows(filepath):\n",
    "    print(f\"üìÇ ƒêang ki·ªÉm tra file: {filepath}\")\n",
    "    try:\n",
    "        # Load d·ªØ li·ªáu\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Th·ªëng k√™ tr∆∞·ªõc khi x√≥a\n",
    "        n_original = len(df)\n",
    "        \n",
    "        # Chuy·ªÉn v·ªÅ string v√† x√≥a kho·∫£ng tr·∫Øng th·ª´a\n",
    "        df['free_text'] = df['free_text'].fillna(\"\").astype(str).str.strip()\n",
    "        \n",
    "        # X√≥a c√°c d√≤ng r·ªóng (length == 0) ho·∫∑c d√≤ng c√≥ gi√° tr·ªã l√† \"nan\" string\n",
    "        df_clean = df[\n",
    "            (df['free_text'] != '') & \n",
    "            (df['free_text'].str.lower() != 'nan') &\n",
    "            (df['free_text'] != 'empty_content') # N·∫øu b∆∞·ªõc tr∆∞·ªõc b·∫°n c√≥ fill placeholder\n",
    "        ]\n",
    "        \n",
    "        n_after = len(df_clean)\n",
    "        n_deleted = n_original - n_after\n",
    "        \n",
    "        if n_deleted > 0:\n",
    "            print(f\"‚ö†Ô∏è ƒê√£ ph√°t hi·ªán v√† x√≥a {n_deleted} d√≤ng r·ªóng.\")\n",
    "            # L∆∞u ƒë√® l·∫°i file\n",
    "            df_clean.to_csv(filepath, index=False)\n",
    "            print(f\"‚úÖ ƒê√£ c·∫≠p nh·∫≠t file: {filepath}\")\n",
    "        else:\n",
    "            print(\"‚úÖ File s·∫°ch, kh√¥ng c√≥ d√≤ng r·ªóng n√†o.\")\n",
    "            \n",
    "        print(f\"   - S·ªë l∆∞·ª£ng d√≤ng c√≤n l·∫°i: {n_after}\")\n",
    "        return df_clean\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file {filepath}\")\n",
    "\n",
    "# --- TH·ª∞C THI CHO T·∫¨P TRAIN ---\n",
    "# (B·∫°n c≈©ng c√≥ th·ªÉ ch·∫°y cho Dev/Test n·∫øu mu·ªën ch·∫Øc ch·∫Øn)\n",
    "df_train_final = remove_empty_rows(TRAIN_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
