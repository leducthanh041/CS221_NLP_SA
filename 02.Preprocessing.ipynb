{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26fbd167",
   "metadata": {},
   "source": [
    "# Load ng·ªØ li·ªáu v√† t√°ch t·ª´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1316e9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ƒêang ƒë·ªçc d·ªØ li·ªáu g·ªëc...\n",
      "‚è≥ ƒêang th·ª±c hi·ªán t√°ch t·ª´ (PyVi)...\n",
      "‚úÖ ƒê√£ t√°ch t·ª´ xong.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "INPUT_DIR = \"./UIT-ViHSD\" # Folder g·ªëc\n",
    "OUTPUT_DIR = \"./UIT-ViHSD-preprocessed\" # Folder ƒë√≠ch\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# --- LOAD D·ªÆ LI·ªÜU G·ªêC ---\n",
    "print(\"‚è≥ ƒêang ƒë·ªçc d·ªØ li·ªáu g·ªëc...\")\n",
    "df_train = pd.read_csv(os.path.join(INPUT_DIR, \"train.csv\"))\n",
    "df_dev = pd.read_csv(os.path.join(INPUT_DIR, \"dev.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(INPUT_DIR, \"test.csv\"))\n",
    "\n",
    "# X·ª≠ l√Ω NaN ban ƒë·∫ßu (b·∫Øt bu·ªôc ƒë·ªÉ code kh√¥ng l·ªói)\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df.dropna(subset=['free_text'], inplace=True)\n",
    "    df['free_text'] = df['free_text'].astype(str)\n",
    "\n",
    "# --- T√ÅCH T·ª™ (SEGMENTATION) ---\n",
    "print(\"‚è≥ ƒêang th·ª±c hi·ªán t√°ch t·ª´ (PyVi)...\")\n",
    "def safe_segment(text):\n",
    "    try:\n",
    "        return ViTokenizer.tokenize(text)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# √Åp d·ª•ng\n",
    "df_train['free_text'] = df_train['free_text'].apply(safe_segment)\n",
    "df_dev['free_text'] = df_dev['free_text'].apply(safe_segment)\n",
    "df_test['free_text'] = df_test['free_text'].apply(safe_segment)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ t√°ch t·ª´ xong.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a1fae",
   "metadata": {},
   "source": [
    "# Lo·∫°i b·ªè Stopword v·ªõi c∆° ch·∫ø \"Safety Check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19cef8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ load 1942 stopwords.\n",
      "‚è≥ ƒêang lo·∫°i b·ªè stopwords (ch·∫ø ƒë·ªô an to√†n)...\n",
      "‚úÖ Ho√†n t·∫•t.\n"
     ]
    }
   ],
   "source": [
    "# --- C·∫§U H√åNH STOPWORDS ---\n",
    "STOPWORD_FILE = \"vietnamese-stopwords.txt\" \n",
    "\n",
    "# Load stopwords\n",
    "stopword_set = set()\n",
    "try:\n",
    "    with open(STOPWORD_FILE, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word = line.strip()\n",
    "            if word:\n",
    "                # Chuy·ªÉn stopword sang d·∫°ng segmented ƒë·ªÉ kh·ªõp d·ªØ li·ªáu (v√≠ d·ª•: \"b·ªüi v√¨\" -> \"b·ªüi_v√¨\")\n",
    "                stopword_set.add(word.replace(' ', '_'))\n",
    "    print(f\"‚úÖ ƒê√£ load {len(stopword_set)} stopwords.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file stopwords. B·ªè qua b∆∞·ªõc n√†y.\")\n",
    "\n",
    "# --- H√ÄM LO·∫†I B·ªé AN TO√ÄN ---\n",
    "def remove_stopword_safe(text, stop_set):\n",
    "    if not text: return \"\"\n",
    "    \n",
    "    tokens = text.split()\n",
    "    # Gi·ªØ l·∫°i c√°c t·ª´ KH√îNG n·∫±m trong danh s√°ch stopword\n",
    "    clean_tokens = [t for t in tokens if t.lower() not in stop_set]\n",
    "    \n",
    "    # K·ª∏ THU·∫¨T AN TO√ÄN:\n",
    "    # N·∫øu x√≥a xong m√† r·ªóng (t·ª©c l√† c√¢u to√†n stopword) -> Tr·∫£ v·ªÅ c√¢u g·ªëc (tokens ban ƒë·∫ßu)\n",
    "    # N·∫øu c√≤n t·ª´ -> Tr·∫£ v·ªÅ c√¢u ƒë√£ clean\n",
    "    if len(clean_tokens) == 0:\n",
    "        return text # <--- GI·ªÆ L·∫†I G·ªêC\n",
    "    \n",
    "    return \" \".join(clean_tokens)\n",
    "\n",
    "# --- TH·ª∞C THI ---\n",
    "if len(stopword_set) > 0:\n",
    "    print(\"‚è≥ ƒêang lo·∫°i b·ªè stopwords (ch·∫ø ƒë·ªô an to√†n)...\")\n",
    "    df_train['free_text'] = df_train['free_text'].apply(lambda x: remove_stopword_safe(x, stopword_set))\n",
    "    df_dev['free_text'] = df_dev['free_text'].apply(lambda x: remove_stopword_safe(x, stopword_set))\n",
    "    df_test['free_text'] = df_test['free_text'].apply(lambda x: remove_stopword_safe(x, stopword_set))\n",
    "    print(\"‚úÖ Ho√†n t·∫•t.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbfc7493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- KI·ªÇM TRA D·ªÆ LI·ªÜU R·ªñNG ---\n",
      "üìä [Train] S·ªë d√≤ng r·ªóng: 0\n",
      "üìä [Dev] S·ªë d√≤ng r·ªóng: 0\n",
      "üìä [Test] S·ªë d√≤ng r·ªóng: 0\n",
      "\n",
      "üéâ TUY·ªÜT V·ªúI! Kh√¥ng c√≥ d√≤ng n√†o b·ªã r·ªóng. D·ªØ li·ªáu b·∫£o to√†n 100%.\n"
     ]
    }
   ],
   "source": [
    "def check_empty_rows(df, name):\n",
    "    # ƒê·∫øm s·ªë d√≤ng r·ªóng ho·∫∑c ch·ªâ ch·ª©a kho·∫£ng tr·∫Øng\n",
    "    count = df[df['free_text'].str.strip() == ''].shape[0]\n",
    "    print(f\"üìä [{name}] S·ªë d√≤ng r·ªóng: {count}\")\n",
    "    return count\n",
    "\n",
    "print(\"--- KI·ªÇM TRA D·ªÆ LI·ªÜU R·ªñNG ---\")\n",
    "c1 = check_empty_rows(df_train, \"Train\")\n",
    "c2 = check_empty_rows(df_dev, \"Dev\")\n",
    "c3 = check_empty_rows(df_test, \"Test\")\n",
    "\n",
    "if c1 == 0 and c2 == 0 and c3 == 0:\n",
    "    print(\"\\nüéâ TUY·ªÜT V·ªúI! Kh√¥ng c√≥ d√≤ng n√†o b·ªã r·ªóng. D·ªØ li·ªáu b·∫£o to√†n 100%.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è V·∫´n c√≤n d√≤ng r·ªóng (L·ªói b·∫•t th∆∞·ªùng).\")\n",
    "\n",
    "# L∆∞u t·∫°m k·∫øt qu·∫£ b∆∞·ªõc n√†y\n",
    "df_train.to_csv(os.path.join(OUTPUT_DIR, \"train_step1_stopword.csv\"), index=False)\n",
    "df_dev.to_csv(os.path.join(OUTPUT_DIR, \"dev_step1_stopword.csv\"), index=False)\n",
    "df_test.to_csv(os.path.join(OUTPUT_DIR, \"test_step1_stopword.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b1d83",
   "metadata": {},
   "source": [
    "# Chuy·ªÉn h·∫øt sang lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e1ec213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ƒêang th·ª±c hi·ªán chuy·ªÉn ƒë·ªïi ch·ªØ th∆∞·ªùng (Lowercase)...\n",
      "   - [train] ƒê√£ chuy·ªÉn ƒë·ªïi xong. S·ªë d√≤ng r·ªóng: 0\n",
      "     -> ƒê√£ l∆∞u: train_step2_lower.csv\n",
      "   - [dev] ƒê√£ chuy·ªÉn ƒë·ªïi xong. S·ªë d√≤ng r·ªóng: 0\n",
      "     -> ƒê√£ l∆∞u: dev_step2_lower.csv\n",
      "   - [test] ƒê√£ chuy·ªÉn ƒë·ªïi xong. S·ªë d√≤ng r·ªóng: 0\n",
      "     -> ƒê√£ l∆∞u: test_step2_lower.csv\n",
      "\n",
      "‚úÖ Ho√†n t·∫•t b∆∞·ªõc Lowercase. B·∫°n ƒë√£ s·∫µn s√†ng cho b∆∞·ªõc ti·∫øp theo (X·ª≠ l√Ω Regex).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "DIR = \"./UIT-ViHSD-preprocessed\"\n",
    "\n",
    "# T√™n file ƒë·∫ßu v√†o (K·∫øt qu·∫£ c·ªßa b∆∞·ªõc Stopwords v·ª´a ch·∫°y)\n",
    "INPUT_FILES = {\n",
    "    'train': \"train_step1_stopword.csv\",\n",
    "    'dev': \"dev_step1_stopword.csv\",\n",
    "    'test': \"test_step1_stopword.csv\"\n",
    "}\n",
    "\n",
    "# T√™n file ƒë·∫ßu ra\n",
    "OUTPUT_FILES = {\n",
    "    'train': \"train_step2_lower.csv\",\n",
    "    'dev': \"dev_step2_lower.csv\",\n",
    "    'test': \"test_step2_lower.csv\"\n",
    "}\n",
    "\n",
    "def to_lowercase_and_save():\n",
    "    print(\"‚è≥ ƒêang th·ª±c hi·ªán chuy·ªÉn ƒë·ªïi ch·ªØ th∆∞·ªùng (Lowercase)...\")\n",
    "    \n",
    "    for split_name, file_name in INPUT_FILES.items():\n",
    "        input_path = os.path.join(DIR, file_name)\n",
    "        \n",
    "        # Ki·ªÉm tra file t·ªìn t·∫°i\n",
    "        if not os.path.exists(input_path):\n",
    "            print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file {file_name}. Vui l√≤ng ch·∫°y l·∫°i b∆∞·ªõc Stopwords tr∆∞·ªõc.\")\n",
    "            continue\n",
    "            \n",
    "        # 1. ƒê·ªçc d·ªØ li·ªáu\n",
    "        df = pd.read_csv(input_path)\n",
    "        \n",
    "        # X·ª≠ l√Ω NaN (chuy·ªÉn th√†nh chu·ªói r·ªóng ƒë·ªÉ kh√¥ng l·ªói, d√π b∆∞·ªõc tr∆∞·ªõc ƒë√£ x·ª≠ l√Ω)\n",
    "        df['free_text'] = df['free_text'].fillna(\"\").astype(str)\n",
    "        \n",
    "        # 2. Chuy·ªÉn ch·ªØ th∆∞·ªùng\n",
    "        # ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng nh·∫•t\n",
    "        df['free_text'] = df['free_text'].str.lower()\n",
    "        \n",
    "        # 3. Ki·ªÉm tra l·∫°i r·ªóng (Sanity Check)\n",
    "        empty_count = df[df['free_text'].str.strip() == ''].shape[0]\n",
    "        print(f\"   - [{split_name}] ƒê√£ chuy·ªÉn ƒë·ªïi xong. S·ªë d√≤ng r·ªóng: {empty_count}\")\n",
    "        \n",
    "        # 4. L∆∞u file\n",
    "        output_path = os.path.join(DIR, OUTPUT_FILES[split_name])\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"     -> ƒê√£ l∆∞u: {OUTPUT_FILES[split_name]}\")\n",
    "\n",
    "# --- TH·ª∞C THI ---\n",
    "to_lowercase_and_save()\n",
    "\n",
    "print(\"\\n‚úÖ Ho√†n t·∫•t b∆∞·ªõc Lowercase. B·∫°n ƒë√£ s·∫µn s√†ng cho b∆∞·ªõc ti·∫øp theo (X·ª≠ l√Ω Regex).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab19522c",
   "metadata": {},
   "source": [
    "# Lo·∫°i b·ªè url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4c8eaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ b∆∞·ªõc Lowercase...\n",
      "\n",
      "üëâ B∆Ø·ªöC 1: LO·∫†I B·ªé URL (http://..., www...)\n",
      "üìä Ki·ªÉm tra d·ªØ li·ªáu r·ªóng sau khi b·ªè URL:\n",
      "   - Train: 0 d√≤ng r·ªóng\n",
      "   - Dev:   0 d√≤ng r·ªóng\n",
      "   - Test:  0 d√≤ng r·ªóng\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "DIR = \"./UIT-ViHSD-preprocessed\"\n",
    "\n",
    "# Load d·ªØ li·ªáu t·ª´ b∆∞·ªõc tr∆∞·ªõc (Lowercase)\n",
    "print(\"‚è≥ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ b∆∞·ªõc Lowercase...\")\n",
    "df_train = pd.read_csv(os.path.join(DIR, \"train_step2_lower.csv\"))\n",
    "df_dev = pd.read_csv(os.path.join(DIR, \"dev_step2_lower.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(DIR, \"test_step2_lower.csv\"))\n",
    "\n",
    "# ƒê·∫£m b·∫£o kh√¥ng l·ªói NaN\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df['free_text'] = df['free_text'].fillna(\"\").astype(str)\n",
    "\n",
    "# --- X·ª¨ L√ù URL ---\n",
    "print(\"\\nüëâ B∆Ø·ªöC 1: LO·∫†I B·ªé URL (http://..., www...)\")\n",
    "\n",
    "def remove_url(text):\n",
    "    # Regex b·∫Øt c√°c ƒë∆∞·ªùng link\n",
    "    return re.sub(r'http\\S+|www\\.\\S+', '', text).strip()\n",
    "\n",
    "# √Åp d·ª•ng\n",
    "df_train['free_text'] = df_train['free_text'].apply(remove_url)\n",
    "df_dev['free_text'] = df_dev['free_text'].apply(remove_url)\n",
    "df_test['free_text'] = df_test['free_text'].apply(remove_url)\n",
    "\n",
    "# --- KI·ªÇM TRA R·ªñNG ---\n",
    "def check_empty(df, name):\n",
    "    count = df[df['free_text'].str.strip() == ''].shape[0]\n",
    "    return count\n",
    "\n",
    "print(\"üìä Ki·ªÉm tra d·ªØ li·ªáu r·ªóng sau khi b·ªè URL:\")\n",
    "e1 = check_empty(df_train, \"Train\")\n",
    "e2 = check_empty(df_dev, \"Dev\")\n",
    "e3 = check_empty(df_test, \"Test\")\n",
    "\n",
    "print(f\"   - Train: {e1} d√≤ng r·ªóng\")\n",
    "print(f\"   - Dev:   {e2} d√≤ng r·ªóng\")\n",
    "print(f\"   - Test:  {e3} d√≤ng r·ªóng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a339c",
   "metadata": {},
   "source": [
    "# Lo·∫°i b·ªè hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bd8ca2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üëâ B∆Ø·ªöC 2: LO·∫†I B·ªé K√ù T·ª∞ HASHTAG (X√≥a d·∫•u #, gi·ªØ l·∫°i ch·ªØ)\n",
      "üìä Ki·ªÉm tra d·ªØ li·ªáu r·ªóng sau khi b·ªè Hashtag:\n",
      "   - Train: 0 d√≤ng r·ªóng\n",
      "   - Dev:   0 d√≤ng r·ªóng\n",
      "   - Test:  0 d√≤ng r·ªóng\n"
     ]
    }
   ],
   "source": [
    "# --- X·ª¨ L√ù HASHTAG ---\n",
    "print(\"\\nüëâ B∆Ø·ªöC 2: LO·∫†I B·ªé K√ù T·ª∞ HASHTAG (X√≥a d·∫•u #, gi·ªØ l·∫°i ch·ªØ)\")\n",
    "\n",
    "def remove_hashtag_symbol(text):\n",
    "    # Ch·ªâ thay th·∫ø d·∫•u # b·∫±ng chu·ªói r·ªóng\n",
    "    return text.replace('#', '').strip()\n",
    "\n",
    "# √Åp d·ª•ng\n",
    "df_train['free_text'] = df_train['free_text'].apply(remove_hashtag_symbol)\n",
    "df_dev['free_text'] = df_dev['free_text'].apply(remove_hashtag_symbol)\n",
    "df_test['free_text'] = df_test['free_text'].apply(remove_hashtag_symbol)\n",
    "\n",
    "# --- KI·ªÇM TRA R·ªñNG ---\n",
    "print(\"üìä Ki·ªÉm tra d·ªØ li·ªáu r·ªóng sau khi b·ªè Hashtag:\")\n",
    "e1 = check_empty(df_train, \"Train\")\n",
    "e2 = check_empty(df_dev, \"Dev\")\n",
    "e3 = check_empty(df_test, \"Test\")\n",
    "\n",
    "print(f\"   - Train: {e1} d√≤ng r·ªóng\")\n",
    "print(f\"   - Dev:   {e2} d√≤ng r·ªóng\")\n",
    "print(f\"   - Test:  {e3} d√≤ng r·ªóng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b35cad",
   "metadata": {},
   "source": [
    "# Lo·∫°i b·ªè mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f506a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üëâ B∆Ø·ªöC 3: LO·∫†I B·ªé MENTION TAG (@user)\n",
      "üìä Ki·ªÉm tra d·ªØ li·ªáu r·ªóng sau khi b·ªè Mention (FINAL CHECK):\n",
      "   - Train: 0 d√≤ng r·ªóng\n",
      "   - Dev:   0 d√≤ng r·ªóng\n",
      "   - Test:  0 d√≤ng r·ªóng\n",
      "\n",
      "‚úÖ D·ªØ li·ªáu ho√†n h·∫£o! ƒêang l∆∞u file final...\n",
      "üéâ ƒê√£ ho√†n t·∫•t to√†n b·ªô quy tr√¨nh Preprocessing!\n",
      "üìÇ File k·∫øt qu·∫£: ./UIT-ViHSD-preprocessed\n"
     ]
    }
   ],
   "source": [
    "# --- X·ª¨ L√ù MENTION ---\n",
    "print(\"\\nüëâ B∆Ø·ªöC 3: LO·∫†I B·ªé MENTION TAG (@user)\")\n",
    "\n",
    "def remove_mention(text):\n",
    "    # Regex b·∫Øt @ ƒëi li·ªÅn v·ªõi c√°c k√Ω t·ª± kh√¥ng ph·∫£i kho·∫£ng tr·∫Øng\n",
    "    return re.sub(r'@\\S+', '', text).strip()\n",
    "\n",
    "# √Åp d·ª•ng\n",
    "df_train['free_text'] = df_train['free_text'].apply(remove_mention)\n",
    "df_dev['free_text'] = df_dev['free_text'].apply(remove_mention)\n",
    "df_test['free_text'] = df_test['free_text'].apply(remove_mention)\n",
    "\n",
    "# --- KI·ªÇM TRA R·ªñNG L·∫¶N CU·ªêI ---\n",
    "print(\"üìä Ki·ªÉm tra d·ªØ li·ªáu r·ªóng sau khi b·ªè Mention (FINAL CHECK):\")\n",
    "e1_final = check_empty(df_train, \"Train\")\n",
    "e2_final = check_empty(df_dev, \"Dev\")\n",
    "e3_final = check_empty(df_test, \"Test\")\n",
    "\n",
    "print(f\"   - Train: {e1_final} d√≤ng r·ªóng\")\n",
    "print(f\"   - Dev:   {e2_final} d√≤ng r·ªóng\")\n",
    "print(f\"   - Test:  {e3_final} d√≤ng r·ªóng\")\n",
    "\n",
    "# --- L∆ØU K·∫æT QU·∫¢ CU·ªêI C√ôNG ---\n",
    "if e1_final == 0 and e3_final == 0:\n",
    "    print(\"\\n‚úÖ D·ªØ li·ªáu ho√†n h·∫£o! ƒêang l∆∞u file final...\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è C√≥ m·ªôt s·ªë d√≤ng b·ªã r·ªóng (do c√¢u ch·ªâ ch·ª©a to√†n link/mention). ƒêang l∆∞u file final...\")\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n file chu·∫©n\n",
    "final_train = os.path.join(DIR, \"train.csv\")\n",
    "final_dev = os.path.join(DIR, \"dev.csv\")\n",
    "final_test = os.path.join(DIR, \"test.csv\")\n",
    "\n",
    "# L∆∞u file\n",
    "df_train.to_csv(final_train, index=False)\n",
    "df_dev.to_csv(final_dev, index=False)\n",
    "df_test.to_csv(final_test, index=False)\n",
    "\n",
    "print(f\"üéâ ƒê√£ ho√†n t·∫•t to√†n b·ªô quy tr√¨nh Preprocessing!\")\n",
    "print(f\"üìÇ File k·∫øt qu·∫£: {DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
